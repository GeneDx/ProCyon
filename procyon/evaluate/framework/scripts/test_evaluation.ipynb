{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from logging import getLogger\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from evaluate import load\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import HfArgumentParser\n",
    "\n",
    "\n",
    "from txplm.evaluate.framework.core import run_evaluation\n",
    "from txplm.evaluate.framework.args import EvalArgs\n",
    "\n",
    "from txplm.data.dataset import (\n",
    "    AASeqDataset,\n",
    "    AASeqTextUnifiedDataset,\n",
    ")\n",
    "\n",
    "from txplm.training.training_args_IT import (\n",
    "    DataArgs,\n",
    "    ModelArgs,\n",
    "    postprocess_args,\n",
    ")\n",
    "\n",
    "from txplm.data.data_utils import (\n",
    "    DATA_DIR,\n",
    "    HOME_DIR,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = \"/om2/vast/kellislab/shared/PLM/model_outputs/pretrain/2024-01-17_12:36_LLAMA2_z2_all/checkpoint-87500\"\n",
    "\n",
    "# test_dir will be used to write config files and outputs from evaluations (cached embeddings, metrics CSV, plots)\n",
    "test_dir = \"/your/favorite/test_dir\"\n",
    "\n",
    "inputs_dir = os.path.join(test_dir, \"inputs\")\n",
    "outputs_dir = os.path.join(test_dir, \"inputs\")\n",
    "\n",
    "os.makedirs(inputs_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make configs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create protein subset for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_PROTEINS_FILE = os.path.join(\n",
    "    DATA_DIR, \"integrated_data/v1/protein/protein_info_filtered.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>protein_id</th>\n",
       "      <th>name</th>\n",
       "      <th>entry</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8699</th>\n",
       "      <td>8699</td>\n",
       "      <td>P20916</td>\n",
       "      <td>MAG</td>\n",
       "      <td>MAG_HUMAN</td>\n",
       "      <td>[FUNCTION: Adhesion molecule that mediates int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2887</th>\n",
       "      <td>2887</td>\n",
       "      <td>Q9UGN4</td>\n",
       "      <td>CD300A</td>\n",
       "      <td>CLM8_HUMAN</td>\n",
       "      <td>[FUNCTION: Inhibitory receptor which may contr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7867</th>\n",
       "      <td>7867</td>\n",
       "      <td>A6PVL3</td>\n",
       "      <td>KNCN</td>\n",
       "      <td>KNCN_HUMAN</td>\n",
       "      <td>[FUNCTION: May play a role in stabilizing dens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>515</td>\n",
       "      <td>O75969</td>\n",
       "      <td>AKAP3</td>\n",
       "      <td>AKAP3_HUMAN</td>\n",
       "      <td>[FUNCTION: May function as a regulator of both...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3256</th>\n",
       "      <td>3256</td>\n",
       "      <td>P04632</td>\n",
       "      <td>CAPNS1</td>\n",
       "      <td>CPNS1_HUMAN</td>\n",
       "      <td>[FUNCTION: Regulatory subunit of the calcium-r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index protein_id    name        entry  \\\n",
       "8699   8699     P20916     MAG    MAG_HUMAN   \n",
       "2887   2887     Q9UGN4  CD300A   CLM8_HUMAN   \n",
       "7867   7867     A6PVL3    KNCN   KNCN_HUMAN   \n",
       "515     515     O75969   AKAP3  AKAP3_HUMAN   \n",
       "3256   3256     P04632  CAPNS1  CPNS1_HUMAN   \n",
       "\n",
       "                                               comments  \n",
       "8699  [FUNCTION: Adhesion molecule that mediates int...  \n",
       "2887  [FUNCTION: Inhibitory receptor which may contr...  \n",
       "7867  [FUNCTION: May play a role in stabilizing dens...  \n",
       "515   [FUNCTION: May function as a regulator of both...  \n",
       "3256  [FUNCTION: Regulatory subunit of the calcium-r...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_proteins = pd.read_pickle(ALL_PROTEINS_FILE)\n",
    "subset = all_proteins.sample(n=100)\n",
    "subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.to_pickle(os.path.join(inputs_dir, \"test_protein_subset.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of [\"test\", \"protst\", \"blast_ppi\", \"multitask\"]\n",
    "#  \"test\": run with only TxPLM text -> protein retrieval evaluation\n",
    "#  \"protst\": run with TxPLM and ProtST for text -> protein retrieval evaluation\n",
    "#  \"blast_ppi\": run with TxPLM and BLAST for protein -> protein retrieval evaluation\n",
    "#  \"multitask\": run with TxPLM only, evaluating across all tasks\n",
    "run_type = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_type == \"test\":\n",
    "  data_yml = f\"\"\"it_datasets:\n",
    "    train:\n",
    "      # GO:\n",
    "      - aaseq_type: protein\n",
    "        text_type: go\n",
    "        relations: [process, function, component]\n",
    "        tasks: [qa, caption, retrieval]\n",
    "        splits: [CL_train]\n",
    "    testing:\n",
    "      # DrugBank:\n",
    "      - aaseq_type: protein\n",
    "        text_type: drugbank\n",
    "        relations: [drug_carrier]\n",
    "        tasks: [retrieval]\n",
    "        splits: [CL_train]\n",
    "      - aaseq_type: protein\n",
    "        text_type: drugbank\n",
    "        relations: [drug_carrier]\n",
    "        tasks: [retrieval]\n",
    "        splits: [CL_train]\n",
    "        key_suffix: subset\n",
    "        dataset_args:\n",
    "          num_instruction_examples: 3\n",
    "          text_variant_type: moa_only\n",
    "        eval_args:\n",
    "          target_subset: {os.path.join(inputs_dir, 'test_protein_subset.pkl')}\n",
    "      - aaseq_type: protein\n",
    "        text_type: drugbank\n",
    "        relations: [drug_transporter]\n",
    "        tasks: [retrieval]\n",
    "        splits: [CL_train]\n",
    "        dataset_args:\n",
    "          num_instruction_examples: 3\n",
    "          text_variant_type: moa_only\"\"\"\n",
    "elif run_type == \"blast_ppi\":\n",
    "  data_yml = \"\"\"it_datasets:\n",
    "    testing:\n",
    "      - aaseq_type: protein\n",
    "        text_type: protein\n",
    "        relations: [homology]\n",
    "        tasks: [retrieval]\n",
    "        splits: [CL_train]\n",
    "        dataset_args:\n",
    "          ppi_store_reverse_edges: True\"\"\"\n",
    "elif run_type == \"protst\":\n",
    "  data_yml = f\"\"\"it_datasets:\n",
    "  testing:\n",
    "    # DrugBank:\n",
    "    - aaseq_type: protein\n",
    "      text_type: drugbank\n",
    "      relations: [drug_carrier]\n",
    "      tasks: [retrieval]\n",
    "      splits: [eval_zero_shot, eval_pt_ft]\"\"\"\n",
    "elif run_type == \"multitask\":\n",
    "  data_yml = f\"\"\"it_datasets:\n",
    "  testing:\n",
    "    # DrugBank:\n",
    "    - aaseq_type: protein\n",
    "      text_type: drugbank\n",
    "      relations: [drug_carrier]\n",
    "      tasks: [retrieval, qa, caption]\n",
    "      splits: [eval_zero_shot, eval_pt_ft]\"\"\"\n",
    "\n",
    "with open(os.path.join(inputs_dir, \"tmp_eval_dataset_config.yml\"), \"w\") as fh:\n",
    "    fh.write(data_yml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_type == \"blast_ppi\":\n",
    "  model_yml = f\"\"\"models:\n",
    "      - model_name: BLAST\n",
    "        args:\n",
    "          max_ev: 10\n",
    "      - model_name: TxPLM\n",
    "        args:\n",
    "          checkpoint_dir: {checkpoint_dir}\"\"\"\n",
    "elif run_type == \"protst\":\n",
    "  model_yml = f\"\"\"models:\n",
    "    - model_name: ProtST\n",
    "      args:\n",
    "        max_prompt_len: 128\n",
    "    - model_name: TxPLM\n",
    "      args:\n",
    "        checkpoint_dir: {checkpoint_dir}\"\"\"\n",
    "else:\n",
    "  model_yml = f\"\"\"models:\n",
    "      - model_name: TxPLM\n",
    "        args:\n",
    "          checkpoint_dir: {checkpoint_dir}\"\"\"\n",
    "\n",
    "with open(os.path.join(out_dir, \"model_config.yml\"), \"w\") as fh:\n",
    "    fh.write(model_yml)\n",
    "with open(os.path.join(inputs_dir, \"tmp_eval_model_config.yml\"), \"w\") as fh:\n",
    "    if blast_test:\n",
    "      fh.write(models_yml_blast)\n",
    "    else:\n",
    "      fh.write(models_yml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_config = f\"\"\"# --------------------------------------   EVAL ARGUMENTS   --------------------------------------\n",
    "# Data config\n",
    "it_data_config_yml: {os.path.join(inputs_dir, \"tmp_eval_dataset_config.yml\")}\n",
    "models_config_yml: {os.path.join(inputs_dir, \"tmp_eval_model_config.yml\")}\n",
    "\n",
    "retrieval_use_cached_target_embeddings: True\n",
    "retrieval_eval_all_proteins: True\n",
    "retrieval_top_k_vals: [10, 20, 100]\n",
    "\n",
    "model_args_from_checkpoint: {checkpoint_dir}\n",
    "data_args_from_checkpoint: {checkpoint_dir}\n",
    "\n",
    "output_dir: {outputs_dir}\n",
    "\n",
    "qa_num_samples: 5\n",
    "\n",
    "filter_training_pairs: True\n",
    "model_args_from_checkpoint: {checkpoint_dir}\n",
    "data_args_from_checkpoint: {checkpoint_dir}\n",
    "\n",
    "# --------------------------------------   DATA ARGUMENTS   --------------------------------------\n",
    "# General:\n",
    "use_caption: False\n",
    "\n",
    "# Splitting:\n",
    "go_split_method: \"sample_aware_ontology_go_centric\"\n",
    "val_split_type: \"pt_ft\"\n",
    "\n",
    "# Dataset-specific attributes:\n",
    "go_def_col: \"standard\"\n",
    "\n",
    "# Negative sampling:\n",
    "num_neg_samples_qa: 1\n",
    "negative_sampling_strategy_qa: 'aaseq_only'\n",
    "negative_sampling_strategy_retrieval: 'in_batch'\n",
    "# --------------------------------------  MODEL ARGUMENTS   --------------------------------------\n",
    "protein_encoder_num_params: '35m'\n",
    "freeze_protein_encoder: \"all\"\n",
    "use_aaseq_embeddings: False\n",
    "freeze_aaseq_embeddings: False\n",
    "\n",
    "# Text encoder:\n",
    "use_text_embeddings: False\n",
    "freeze_text_embeddings: False\n",
    "text_encoder_fname: \"biogpt\"\n",
    "max_text_len: 512\n",
    "#freeze_text_encoder: \"all\"\n",
    "\n",
    "# Modeling-specific:\n",
    "ret_token_access: \"last\"\n",
    "train_qa_full_lm: False\n",
    "train_retrieval_lm: False\n",
    "roll_num: 1\"\"\"\n",
    "\n",
    "with open(os.path.join(inputs_dir, \"tmp_eval_config.yml\"), \"w\") as fh:\n",
    "    fh.write(example_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-notebook test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser((EvalArgs, DataArgs, ModelArgs))\n",
    "\n",
    "eval_args, data_args, model_args = parser.parse_yaml_file(os.path.join(inputs_dir, \"tmp_eval_config.yml\"))\n",
    "_, data_args, model_args = postprocess_args(None, data_args, model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ModelArgs from TxPLM checkpoint: /om2/vast/kellislab/shared/PLM/model_outputs/pretrain/2024-01-17_12:36_LLAMA2_z2_all/checkpoint-87500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received training datasets for task qa, will not be used for evaluation (check data config)\n",
      "Received training datasets for task caption, will not be used for evaluation (check data config)\n",
      "Received training datasets for task retrieval, will not be used for evaluation (check data config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task: retrieval dataset: protein_drugbank_drug_carrier N: 486\n",
      "task: retrieval dataset: protein_drugbank_drug_carrier_subset N: 486\n",
      "task: retrieval dataset: protein_drugbank_drug_transporter N: 1812\n",
      "retrieval: evaluating on 3 datasets\n",
      "updating model args DATA_DIR from /n/holystore01/LABS/mzitnik_lab/Lab/PLM -> /om2/vast/kellislab/shared/PLM/\n",
      "updating stale DATA_DIR for model arg: go_embeddings_path\n",
      "updating stale DATA_DIR for model arg: pfam_embeddings_path\n",
      "updating stale DATA_DIR for model arg: drugbank_embeddings_path\n",
      "updating stale DATA_DIR for model arg: reactome_embeddings_path\n",
      "updating stale DATA_DIR for model arg: omim_embeddings_path\n",
      "updating stale DATA_DIR for model arg: ec_embeddings_path\n",
      "updating stale DATA_DIR for model arg: protein_seq_embeddings_path\n",
      "updating stale DATA_DIR for model arg: protein_struct_embeddings_path\n",
      "updating stale DATA_DIR for model arg: protein_embeddings_idmap_path\n",
      "updating stale DATA_DIR for model arg: drug_struct_embeddings_path\n",
      "updating stale DATA_DIR for model arg: domain_embeddings_path\n",
      "updating stale DATA_DIR for model arg: domain_embeddings_idmap_path\n",
      "updating stale DATA_DIR for model arg: mouse_ortholog_embeddings_path\n",
      "updating stale DATA_DIR for model arg: mouse_ortholog_embeddings_idmap_path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:33<00:00, 16.63s/it]\n",
      "Using sep_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight True\n",
      "model.layers.0.self_attn.q_proj.weight True\n",
      "model.layers.0.self_attn.k_proj.weight True\n",
      "model.layers.0.self_attn.v_proj.weight True\n",
      "model.layers.0.self_attn.o_proj.weight True\n",
      "model.layers.0.mlp.gate_proj.weight True\n",
      "model.layers.0.mlp.up_proj.weight True\n",
      "model.layers.0.mlp.down_proj.weight True\n",
      "model.layers.0.input_layernorm.weight True\n",
      "model.layers.0.post_attention_layernorm.weight True\n",
      "model.layers.1.self_attn.q_proj.weight True\n",
      "model.layers.1.self_attn.k_proj.weight True\n",
      "model.layers.1.self_attn.v_proj.weight True\n",
      "model.layers.1.self_attn.o_proj.weight True\n",
      "model.layers.1.mlp.gate_proj.weight True\n",
      "model.layers.1.mlp.up_proj.weight True\n",
      "model.layers.1.mlp.down_proj.weight True\n",
      "model.layers.1.input_layernorm.weight True\n",
      "model.layers.1.post_attention_layernorm.weight True\n",
      "model.layers.2.self_attn.q_proj.weight True\n",
      "model.layers.2.self_attn.k_proj.weight True\n",
      "model.layers.2.self_attn.v_proj.weight True\n",
      "model.layers.2.self_attn.o_proj.weight True\n",
      "model.layers.2.mlp.gate_proj.weight True\n",
      "model.layers.2.mlp.up_proj.weight True\n",
      "model.layers.2.mlp.down_proj.weight True\n",
      "model.layers.2.input_layernorm.weight True\n",
      "model.layers.2.post_attention_layernorm.weight True\n",
      "model.layers.3.self_attn.q_proj.weight True\n",
      "model.layers.3.self_attn.k_proj.weight True\n",
      "model.layers.3.self_attn.v_proj.weight True\n",
      "model.layers.3.self_attn.o_proj.weight True\n",
      "model.layers.3.mlp.gate_proj.weight True\n",
      "model.layers.3.mlp.up_proj.weight True\n",
      "model.layers.3.mlp.down_proj.weight True\n",
      "model.layers.3.input_layernorm.weight True\n",
      "model.layers.3.post_attention_layernorm.weight True\n",
      "model.layers.4.self_attn.q_proj.weight True\n",
      "model.layers.4.self_attn.k_proj.weight True\n",
      "model.layers.4.self_attn.v_proj.weight True\n",
      "model.layers.4.self_attn.o_proj.weight True\n",
      "model.layers.4.mlp.gate_proj.weight True\n",
      "model.layers.4.mlp.up_proj.weight True\n",
      "model.layers.4.mlp.down_proj.weight True\n",
      "model.layers.4.input_layernorm.weight True\n",
      "model.layers.4.post_attention_layernorm.weight True\n",
      "model.layers.5.self_attn.q_proj.weight True\n",
      "model.layers.5.self_attn.k_proj.weight True\n",
      "model.layers.5.self_attn.v_proj.weight True\n",
      "model.layers.5.self_attn.o_proj.weight True\n",
      "model.layers.5.mlp.gate_proj.weight True\n",
      "model.layers.5.mlp.up_proj.weight True\n",
      "model.layers.5.mlp.down_proj.weight True\n",
      "model.layers.5.input_layernorm.weight True\n",
      "model.layers.5.post_attention_layernorm.weight True\n",
      "model.layers.6.self_attn.q_proj.weight True\n",
      "model.layers.6.self_attn.k_proj.weight True\n",
      "model.layers.6.self_attn.v_proj.weight True\n",
      "model.layers.6.self_attn.o_proj.weight True\n",
      "model.layers.6.mlp.gate_proj.weight True\n",
      "model.layers.6.mlp.up_proj.weight True\n",
      "model.layers.6.mlp.down_proj.weight True\n",
      "model.layers.6.input_layernorm.weight True\n",
      "model.layers.6.post_attention_layernorm.weight True\n",
      "model.layers.7.self_attn.q_proj.weight True\n",
      "model.layers.7.self_attn.k_proj.weight True\n",
      "model.layers.7.self_attn.v_proj.weight True\n",
      "model.layers.7.self_attn.o_proj.weight True\n",
      "model.layers.7.mlp.gate_proj.weight True\n",
      "model.layers.7.mlp.up_proj.weight True\n",
      "model.layers.7.mlp.down_proj.weight True\n",
      "model.layers.7.input_layernorm.weight True\n",
      "model.layers.7.post_attention_layernorm.weight True\n",
      "model.layers.8.self_attn.q_proj.weight True\n",
      "model.layers.8.self_attn.k_proj.weight True\n",
      "model.layers.8.self_attn.v_proj.weight True\n",
      "model.layers.8.self_attn.o_proj.weight True\n",
      "model.layers.8.mlp.gate_proj.weight True\n",
      "model.layers.8.mlp.up_proj.weight True\n",
      "model.layers.8.mlp.down_proj.weight True\n",
      "model.layers.8.input_layernorm.weight True\n",
      "model.layers.8.post_attention_layernorm.weight True\n",
      "model.layers.9.self_attn.q_proj.weight True\n",
      "model.layers.9.self_attn.k_proj.weight True\n",
      "model.layers.9.self_attn.v_proj.weight True\n",
      "model.layers.9.self_attn.o_proj.weight True\n",
      "model.layers.9.mlp.gate_proj.weight True\n",
      "model.layers.9.mlp.up_proj.weight True\n",
      "model.layers.9.mlp.down_proj.weight True\n",
      "model.layers.9.input_layernorm.weight True\n",
      "model.layers.9.post_attention_layernorm.weight True\n",
      "model.layers.10.self_attn.q_proj.weight True\n",
      "model.layers.10.self_attn.k_proj.weight True\n",
      "model.layers.10.self_attn.v_proj.weight True\n",
      "model.layers.10.self_attn.o_proj.weight True\n",
      "model.layers.10.mlp.gate_proj.weight True\n",
      "model.layers.10.mlp.up_proj.weight True\n",
      "model.layers.10.mlp.down_proj.weight True\n",
      "model.layers.10.input_layernorm.weight True\n",
      "model.layers.10.post_attention_layernorm.weight True\n",
      "model.layers.11.self_attn.q_proj.weight True\n",
      "model.layers.11.self_attn.k_proj.weight True\n",
      "model.layers.11.self_attn.v_proj.weight True\n",
      "model.layers.11.self_attn.o_proj.weight True\n",
      "model.layers.11.mlp.gate_proj.weight True\n",
      "model.layers.11.mlp.up_proj.weight True\n",
      "model.layers.11.mlp.down_proj.weight True\n",
      "model.layers.11.input_layernorm.weight True\n",
      "model.layers.11.post_attention_layernorm.weight True\n",
      "model.layers.12.self_attn.q_proj.weight True\n",
      "model.layers.12.self_attn.k_proj.weight True\n",
      "model.layers.12.self_attn.v_proj.weight True\n",
      "model.layers.12.self_attn.o_proj.weight True\n",
      "model.layers.12.mlp.gate_proj.weight True\n",
      "model.layers.12.mlp.up_proj.weight True\n",
      "model.layers.12.mlp.down_proj.weight True\n",
      "model.layers.12.input_layernorm.weight True\n",
      "model.layers.12.post_attention_layernorm.weight True\n",
      "model.layers.13.self_attn.q_proj.weight True\n",
      "model.layers.13.self_attn.k_proj.weight True\n",
      "model.layers.13.self_attn.v_proj.weight True\n",
      "model.layers.13.self_attn.o_proj.weight True\n",
      "model.layers.13.mlp.gate_proj.weight True\n",
      "model.layers.13.mlp.up_proj.weight True\n",
      "model.layers.13.mlp.down_proj.weight True\n",
      "model.layers.13.input_layernorm.weight True\n",
      "model.layers.13.post_attention_layernorm.weight True\n",
      "model.layers.14.self_attn.q_proj.weight True\n",
      "model.layers.14.self_attn.k_proj.weight True\n",
      "model.layers.14.self_attn.v_proj.weight True\n",
      "model.layers.14.self_attn.o_proj.weight True\n",
      "model.layers.14.mlp.gate_proj.weight True\n",
      "model.layers.14.mlp.up_proj.weight True\n",
      "model.layers.14.mlp.down_proj.weight True\n",
      "model.layers.14.input_layernorm.weight True\n",
      "model.layers.14.post_attention_layernorm.weight True\n",
      "model.layers.15.self_attn.q_proj.weight True\n",
      "model.layers.15.self_attn.k_proj.weight True\n",
      "model.layers.15.self_attn.v_proj.weight True\n",
      "model.layers.15.self_attn.o_proj.weight True\n",
      "model.layers.15.mlp.gate_proj.weight True\n",
      "model.layers.15.mlp.up_proj.weight True\n",
      "model.layers.15.mlp.down_proj.weight True\n",
      "model.layers.15.input_layernorm.weight True\n",
      "model.layers.15.post_attention_layernorm.weight True\n",
      "model.layers.16.self_attn.q_proj.weight True\n",
      "model.layers.16.self_attn.k_proj.weight True\n",
      "model.layers.16.self_attn.v_proj.weight True\n",
      "model.layers.16.self_attn.o_proj.weight True\n",
      "model.layers.16.mlp.gate_proj.weight True\n",
      "model.layers.16.mlp.up_proj.weight True\n",
      "model.layers.16.mlp.down_proj.weight True\n",
      "model.layers.16.input_layernorm.weight True\n",
      "model.layers.16.post_attention_layernorm.weight True\n",
      "model.layers.17.self_attn.q_proj.weight True\n",
      "model.layers.17.self_attn.k_proj.weight True\n",
      "model.layers.17.self_attn.v_proj.weight True\n",
      "model.layers.17.self_attn.o_proj.weight True\n",
      "model.layers.17.mlp.gate_proj.weight True\n",
      "model.layers.17.mlp.up_proj.weight True\n",
      "model.layers.17.mlp.down_proj.weight True\n",
      "model.layers.17.input_layernorm.weight True\n",
      "model.layers.17.post_attention_layernorm.weight True\n",
      "model.layers.18.self_attn.q_proj.weight True\n",
      "model.layers.18.self_attn.k_proj.weight True\n",
      "model.layers.18.self_attn.v_proj.weight True\n",
      "model.layers.18.self_attn.o_proj.weight True\n",
      "model.layers.18.mlp.gate_proj.weight True\n",
      "model.layers.18.mlp.up_proj.weight True\n",
      "model.layers.18.mlp.down_proj.weight True\n",
      "model.layers.18.input_layernorm.weight True\n",
      "model.layers.18.post_attention_layernorm.weight True\n",
      "model.layers.19.self_attn.q_proj.weight True\n",
      "model.layers.19.self_attn.k_proj.weight True\n",
      "model.layers.19.self_attn.v_proj.weight True\n",
      "model.layers.19.self_attn.o_proj.weight True\n",
      "model.layers.19.mlp.gate_proj.weight True\n",
      "model.layers.19.mlp.up_proj.weight True\n",
      "model.layers.19.mlp.down_proj.weight True\n",
      "model.layers.19.input_layernorm.weight True\n",
      "model.layers.19.post_attention_layernorm.weight True\n",
      "model.layers.20.self_attn.q_proj.weight True\n",
      "model.layers.20.self_attn.k_proj.weight True\n",
      "model.layers.20.self_attn.v_proj.weight True\n",
      "model.layers.20.self_attn.o_proj.weight True\n",
      "model.layers.20.mlp.gate_proj.weight True\n",
      "model.layers.20.mlp.up_proj.weight True\n",
      "model.layers.20.mlp.down_proj.weight True\n",
      "model.layers.20.input_layernorm.weight True\n",
      "model.layers.20.post_attention_layernorm.weight True\n",
      "model.layers.21.self_attn.q_proj.weight True\n",
      "model.layers.21.self_attn.k_proj.weight True\n",
      "model.layers.21.self_attn.v_proj.weight True\n",
      "model.layers.21.self_attn.o_proj.weight True\n",
      "model.layers.21.mlp.gate_proj.weight True\n",
      "model.layers.21.mlp.up_proj.weight True\n",
      "model.layers.21.mlp.down_proj.weight True\n",
      "model.layers.21.input_layernorm.weight True\n",
      "model.layers.21.post_attention_layernorm.weight True\n",
      "model.layers.22.self_attn.q_proj.weight True\n",
      "model.layers.22.self_attn.k_proj.weight True\n",
      "model.layers.22.self_attn.v_proj.weight True\n",
      "model.layers.22.self_attn.o_proj.weight True\n",
      "model.layers.22.mlp.gate_proj.weight True\n",
      "model.layers.22.mlp.up_proj.weight True\n",
      "model.layers.22.mlp.down_proj.weight True\n",
      "model.layers.22.input_layernorm.weight True\n",
      "model.layers.22.post_attention_layernorm.weight True\n",
      "model.layers.23.self_attn.q_proj.weight True\n",
      "model.layers.23.self_attn.k_proj.weight True\n",
      "model.layers.23.self_attn.v_proj.weight True\n",
      "model.layers.23.self_attn.o_proj.weight True\n",
      "model.layers.23.mlp.gate_proj.weight True\n",
      "model.layers.23.mlp.up_proj.weight True\n",
      "model.layers.23.mlp.down_proj.weight True\n",
      "model.layers.23.input_layernorm.weight True\n",
      "model.layers.23.post_attention_layernorm.weight True\n",
      "model.layers.24.self_attn.q_proj.weight True\n",
      "model.layers.24.self_attn.k_proj.weight True\n",
      "model.layers.24.self_attn.v_proj.weight True\n",
      "model.layers.24.self_attn.o_proj.weight True\n",
      "model.layers.24.mlp.gate_proj.weight True\n",
      "model.layers.24.mlp.up_proj.weight True\n",
      "model.layers.24.mlp.down_proj.weight True\n",
      "model.layers.24.input_layernorm.weight True\n",
      "model.layers.24.post_attention_layernorm.weight True\n",
      "model.layers.25.self_attn.q_proj.weight True\n",
      "model.layers.25.self_attn.k_proj.weight True\n",
      "model.layers.25.self_attn.v_proj.weight True\n",
      "model.layers.25.self_attn.o_proj.weight True\n",
      "model.layers.25.mlp.gate_proj.weight True\n",
      "model.layers.25.mlp.up_proj.weight True\n",
      "model.layers.25.mlp.down_proj.weight True\n",
      "model.layers.25.input_layernorm.weight True\n",
      "model.layers.25.post_attention_layernorm.weight True\n",
      "model.layers.26.self_attn.q_proj.weight True\n",
      "model.layers.26.self_attn.k_proj.weight True\n",
      "model.layers.26.self_attn.v_proj.weight True\n",
      "model.layers.26.self_attn.o_proj.weight True\n",
      "model.layers.26.mlp.gate_proj.weight True\n",
      "model.layers.26.mlp.up_proj.weight True\n",
      "model.layers.26.mlp.down_proj.weight True\n",
      "model.layers.26.input_layernorm.weight True\n",
      "model.layers.26.post_attention_layernorm.weight True\n",
      "model.layers.27.self_attn.q_proj.weight True\n",
      "model.layers.27.self_attn.k_proj.weight True\n",
      "model.layers.27.self_attn.v_proj.weight True\n",
      "model.layers.27.self_attn.o_proj.weight True\n",
      "model.layers.27.mlp.gate_proj.weight True\n",
      "model.layers.27.mlp.up_proj.weight True\n",
      "model.layers.27.mlp.down_proj.weight True\n",
      "model.layers.27.input_layernorm.weight True\n",
      "model.layers.27.post_attention_layernorm.weight True\n",
      "model.layers.28.self_attn.q_proj.weight True\n",
      "model.layers.28.self_attn.k_proj.weight True\n",
      "model.layers.28.self_attn.v_proj.weight True\n",
      "model.layers.28.self_attn.o_proj.weight True\n",
      "model.layers.28.mlp.gate_proj.weight True\n",
      "model.layers.28.mlp.up_proj.weight True\n",
      "model.layers.28.mlp.down_proj.weight True\n",
      "model.layers.28.input_layernorm.weight True\n",
      "model.layers.28.post_attention_layernorm.weight True\n",
      "model.layers.29.self_attn.q_proj.weight True\n",
      "model.layers.29.self_attn.k_proj.weight True\n",
      "model.layers.29.self_attn.v_proj.weight True\n",
      "model.layers.29.self_attn.o_proj.weight True\n",
      "model.layers.29.mlp.gate_proj.weight True\n",
      "model.layers.29.mlp.up_proj.weight True\n",
      "model.layers.29.mlp.down_proj.weight True\n",
      "model.layers.29.input_layernorm.weight True\n",
      "model.layers.29.post_attention_layernorm.weight True\n",
      "model.layers.30.self_attn.q_proj.weight True\n",
      "model.layers.30.self_attn.k_proj.weight True\n",
      "model.layers.30.self_attn.v_proj.weight True\n",
      "model.layers.30.self_attn.o_proj.weight True\n",
      "model.layers.30.mlp.gate_proj.weight True\n",
      "model.layers.30.mlp.up_proj.weight True\n",
      "model.layers.30.mlp.down_proj.weight True\n",
      "model.layers.30.input_layernorm.weight True\n",
      "model.layers.30.post_attention_layernorm.weight True\n",
      "model.layers.31.self_attn.q_proj.weight True\n",
      "model.layers.31.self_attn.k_proj.weight True\n",
      "model.layers.31.self_attn.v_proj.weight True\n",
      "model.layers.31.self_attn.o_proj.weight True\n",
      "model.layers.31.mlp.gate_proj.weight True\n",
      "model.layers.31.mlp.up_proj.weight True\n",
      "model.layers.31.mlp.down_proj.weight True\n",
      "model.layers.31.input_layernorm.weight True\n",
      "model.layers.31.post_attention_layernorm.weight True\n",
      "model.norm.weight True\n",
      "lm_head.weight True\n",
      "model.embed_tokens.weight True\n",
      "model.layers.0.self_attn.q_proj.weight True\n",
      "model.layers.0.self_attn.k_proj.weight True\n",
      "model.layers.0.self_attn.v_proj.weight True\n",
      "model.layers.0.self_attn.o_proj.weight True\n",
      "model.layers.0.mlp.gate_proj.weight True\n",
      "model.layers.0.mlp.up_proj.weight True\n",
      "model.layers.0.mlp.down_proj.weight True\n",
      "model.layers.0.input_layernorm.weight True\n",
      "model.layers.0.post_attention_layernorm.weight True\n",
      "model.layers.1.self_attn.q_proj.weight True\n",
      "model.layers.1.self_attn.k_proj.weight True\n",
      "model.layers.1.self_attn.v_proj.weight True\n",
      "model.layers.1.self_attn.o_proj.weight True\n",
      "model.layers.1.mlp.gate_proj.weight True\n",
      "model.layers.1.mlp.up_proj.weight True\n",
      "model.layers.1.mlp.down_proj.weight True\n",
      "model.layers.1.input_layernorm.weight True\n",
      "model.layers.1.post_attention_layernorm.weight True\n",
      "model.layers.2.self_attn.q_proj.weight True\n",
      "model.layers.2.self_attn.k_proj.weight True\n",
      "model.layers.2.self_attn.v_proj.weight True\n",
      "model.layers.2.self_attn.o_proj.weight True\n",
      "model.layers.2.mlp.gate_proj.weight True\n",
      "model.layers.2.mlp.up_proj.weight True\n",
      "model.layers.2.mlp.down_proj.weight True\n",
      "model.layers.2.input_layernorm.weight True\n",
      "model.layers.2.post_attention_layernorm.weight True\n",
      "model.layers.3.self_attn.q_proj.weight True\n",
      "model.layers.3.self_attn.k_proj.weight True\n",
      "model.layers.3.self_attn.v_proj.weight True\n",
      "model.layers.3.self_attn.o_proj.weight True\n",
      "model.layers.3.mlp.gate_proj.weight True\n",
      "model.layers.3.mlp.up_proj.weight True\n",
      "model.layers.3.mlp.down_proj.weight True\n",
      "model.layers.3.input_layernorm.weight True\n",
      "model.layers.3.post_attention_layernorm.weight True\n",
      "model.layers.4.self_attn.q_proj.weight True\n",
      "model.layers.4.self_attn.k_proj.weight True\n",
      "model.layers.4.self_attn.v_proj.weight True\n",
      "model.layers.4.self_attn.o_proj.weight True\n",
      "model.layers.4.mlp.gate_proj.weight True\n",
      "model.layers.4.mlp.up_proj.weight True\n",
      "model.layers.4.mlp.down_proj.weight True\n",
      "model.layers.4.input_layernorm.weight True\n",
      "model.layers.4.post_attention_layernorm.weight True\n",
      "model.layers.5.self_attn.q_proj.weight True\n",
      "model.layers.5.self_attn.k_proj.weight True\n",
      "model.layers.5.self_attn.v_proj.weight True\n",
      "model.layers.5.self_attn.o_proj.weight True\n",
      "model.layers.5.mlp.gate_proj.weight True\n",
      "model.layers.5.mlp.up_proj.weight True\n",
      "model.layers.5.mlp.down_proj.weight True\n",
      "model.layers.5.input_layernorm.weight True\n",
      "model.layers.5.post_attention_layernorm.weight True\n",
      "model.layers.6.self_attn.q_proj.weight True\n",
      "model.layers.6.self_attn.k_proj.weight True\n",
      "model.layers.6.self_attn.v_proj.weight True\n",
      "model.layers.6.self_attn.o_proj.weight True\n",
      "model.layers.6.mlp.gate_proj.weight True\n",
      "model.layers.6.mlp.up_proj.weight True\n",
      "model.layers.6.mlp.down_proj.weight True\n",
      "model.layers.6.input_layernorm.weight True\n",
      "model.layers.6.post_attention_layernorm.weight True\n",
      "model.layers.7.self_attn.q_proj.weight True\n",
      "model.layers.7.self_attn.k_proj.weight True\n",
      "model.layers.7.self_attn.v_proj.weight True\n",
      "model.layers.7.self_attn.o_proj.weight True\n",
      "model.layers.7.mlp.gate_proj.weight True\n",
      "model.layers.7.mlp.up_proj.weight True\n",
      "model.layers.7.mlp.down_proj.weight True\n",
      "model.layers.7.input_layernorm.weight True\n",
      "model.layers.7.post_attention_layernorm.weight True\n",
      "model.layers.8.self_attn.q_proj.weight True\n",
      "model.layers.8.self_attn.k_proj.weight True\n",
      "model.layers.8.self_attn.v_proj.weight True\n",
      "model.layers.8.self_attn.o_proj.weight True\n",
      "model.layers.8.mlp.gate_proj.weight True\n",
      "model.layers.8.mlp.up_proj.weight True\n",
      "model.layers.8.mlp.down_proj.weight True\n",
      "model.layers.8.input_layernorm.weight True\n",
      "model.layers.8.post_attention_layernorm.weight True\n",
      "model.layers.9.self_attn.q_proj.weight True\n",
      "model.layers.9.self_attn.k_proj.weight True\n",
      "model.layers.9.self_attn.v_proj.weight True\n",
      "model.layers.9.self_attn.o_proj.weight True\n",
      "model.layers.9.mlp.gate_proj.weight True\n",
      "model.layers.9.mlp.up_proj.weight True\n",
      "model.layers.9.mlp.down_proj.weight True\n",
      "model.layers.9.input_layernorm.weight True\n",
      "model.layers.9.post_attention_layernorm.weight True\n",
      "model.layers.10.self_attn.q_proj.weight True\n",
      "model.layers.10.self_attn.k_proj.weight True\n",
      "model.layers.10.self_attn.v_proj.weight True\n",
      "model.layers.10.self_attn.o_proj.weight True\n",
      "model.layers.10.mlp.gate_proj.weight True\n",
      "model.layers.10.mlp.up_proj.weight True\n",
      "model.layers.10.mlp.down_proj.weight True\n",
      "model.layers.10.input_layernorm.weight True\n",
      "model.layers.10.post_attention_layernorm.weight True\n",
      "model.layers.11.self_attn.q_proj.weight True\n",
      "model.layers.11.self_attn.k_proj.weight True\n",
      "model.layers.11.self_attn.v_proj.weight True\n",
      "model.layers.11.self_attn.o_proj.weight True\n",
      "model.layers.11.mlp.gate_proj.weight True\n",
      "model.layers.11.mlp.up_proj.weight True\n",
      "model.layers.11.mlp.down_proj.weight True\n",
      "model.layers.11.input_layernorm.weight True\n",
      "model.layers.11.post_attention_layernorm.weight True\n",
      "model.layers.12.self_attn.q_proj.weight True\n",
      "model.layers.12.self_attn.k_proj.weight True\n",
      "model.layers.12.self_attn.v_proj.weight True\n",
      "model.layers.12.self_attn.o_proj.weight True\n",
      "model.layers.12.mlp.gate_proj.weight True\n",
      "model.layers.12.mlp.up_proj.weight True\n",
      "model.layers.12.mlp.down_proj.weight True\n",
      "model.layers.12.input_layernorm.weight True\n",
      "model.layers.12.post_attention_layernorm.weight True\n",
      "model.layers.13.self_attn.q_proj.weight True\n",
      "model.layers.13.self_attn.k_proj.weight True\n",
      "model.layers.13.self_attn.v_proj.weight True\n",
      "model.layers.13.self_attn.o_proj.weight True\n",
      "model.layers.13.mlp.gate_proj.weight True\n",
      "model.layers.13.mlp.up_proj.weight True\n",
      "model.layers.13.mlp.down_proj.weight True\n",
      "model.layers.13.input_layernorm.weight True\n",
      "model.layers.13.post_attention_layernorm.weight True\n",
      "model.layers.14.self_attn.q_proj.weight True\n",
      "model.layers.14.self_attn.k_proj.weight True\n",
      "model.layers.14.self_attn.v_proj.weight True\n",
      "model.layers.14.self_attn.o_proj.weight True\n",
      "model.layers.14.mlp.gate_proj.weight True\n",
      "model.layers.14.mlp.up_proj.weight True\n",
      "model.layers.14.mlp.down_proj.weight True\n",
      "model.layers.14.input_layernorm.weight True\n",
      "model.layers.14.post_attention_layernorm.weight True\n",
      "model.layers.15.self_attn.q_proj.weight True\n",
      "model.layers.15.self_attn.k_proj.weight True\n",
      "model.layers.15.self_attn.v_proj.weight True\n",
      "model.layers.15.self_attn.o_proj.weight True\n",
      "model.layers.15.mlp.gate_proj.weight True\n",
      "model.layers.15.mlp.up_proj.weight True\n",
      "model.layers.15.mlp.down_proj.weight True\n",
      "model.layers.15.input_layernorm.weight True\n",
      "model.layers.15.post_attention_layernorm.weight True\n",
      "model.layers.16.self_attn.q_proj.weight True\n",
      "model.layers.16.self_attn.k_proj.weight True\n",
      "model.layers.16.self_attn.v_proj.weight True\n",
      "model.layers.16.self_attn.o_proj.weight True\n",
      "model.layers.16.mlp.gate_proj.weight True\n",
      "model.layers.16.mlp.up_proj.weight True\n",
      "model.layers.16.mlp.down_proj.weight True\n",
      "model.layers.16.input_layernorm.weight True\n",
      "model.layers.16.post_attention_layernorm.weight True\n",
      "model.layers.17.self_attn.q_proj.weight True\n",
      "model.layers.17.self_attn.k_proj.weight True\n",
      "model.layers.17.self_attn.v_proj.weight True\n",
      "model.layers.17.self_attn.o_proj.weight True\n",
      "model.layers.17.mlp.gate_proj.weight True\n",
      "model.layers.17.mlp.up_proj.weight True\n",
      "model.layers.17.mlp.down_proj.weight True\n",
      "model.layers.17.input_layernorm.weight True\n",
      "model.layers.17.post_attention_layernorm.weight True\n",
      "model.layers.18.self_attn.q_proj.weight True\n",
      "model.layers.18.self_attn.k_proj.weight True\n",
      "model.layers.18.self_attn.v_proj.weight True\n",
      "model.layers.18.self_attn.o_proj.weight True\n",
      "model.layers.18.mlp.gate_proj.weight True\n",
      "model.layers.18.mlp.up_proj.weight True\n",
      "model.layers.18.mlp.down_proj.weight True\n",
      "model.layers.18.input_layernorm.weight True\n",
      "model.layers.18.post_attention_layernorm.weight True\n",
      "model.layers.19.self_attn.q_proj.weight True\n",
      "model.layers.19.self_attn.k_proj.weight True\n",
      "model.layers.19.self_attn.v_proj.weight True\n",
      "model.layers.19.self_attn.o_proj.weight True\n",
      "model.layers.19.mlp.gate_proj.weight True\n",
      "model.layers.19.mlp.up_proj.weight True\n",
      "model.layers.19.mlp.down_proj.weight True\n",
      "model.layers.19.input_layernorm.weight True\n",
      "model.layers.19.post_attention_layernorm.weight True\n",
      "model.layers.20.self_attn.q_proj.weight True\n",
      "model.layers.20.self_attn.k_proj.weight True\n",
      "model.layers.20.self_attn.v_proj.weight True\n",
      "model.layers.20.self_attn.o_proj.weight True\n",
      "model.layers.20.mlp.gate_proj.weight True\n",
      "model.layers.20.mlp.up_proj.weight True\n",
      "model.layers.20.mlp.down_proj.weight True\n",
      "model.layers.20.input_layernorm.weight True\n",
      "model.layers.20.post_attention_layernorm.weight True\n",
      "model.layers.21.self_attn.q_proj.weight True\n",
      "model.layers.21.self_attn.k_proj.weight True\n",
      "model.layers.21.self_attn.v_proj.weight True\n",
      "model.layers.21.self_attn.o_proj.weight True\n",
      "model.layers.21.mlp.gate_proj.weight True\n",
      "model.layers.21.mlp.up_proj.weight True\n",
      "model.layers.21.mlp.down_proj.weight True\n",
      "model.layers.21.input_layernorm.weight True\n",
      "model.layers.21.post_attention_layernorm.weight True\n",
      "model.layers.22.self_attn.q_proj.weight True\n",
      "model.layers.22.self_attn.k_proj.weight True\n",
      "model.layers.22.self_attn.v_proj.weight True\n",
      "model.layers.22.self_attn.o_proj.weight True\n",
      "model.layers.22.mlp.gate_proj.weight True\n",
      "model.layers.22.mlp.up_proj.weight True\n",
      "model.layers.22.mlp.down_proj.weight True\n",
      "model.layers.22.input_layernorm.weight True\n",
      "model.layers.22.post_attention_layernorm.weight True\n",
      "model.layers.23.self_attn.q_proj.weight True\n",
      "model.layers.23.self_attn.k_proj.weight True\n",
      "model.layers.23.self_attn.v_proj.weight True\n",
      "model.layers.23.self_attn.o_proj.weight True\n",
      "model.layers.23.mlp.gate_proj.weight True\n",
      "model.layers.23.mlp.up_proj.weight True\n",
      "model.layers.23.mlp.down_proj.weight True\n",
      "model.layers.23.input_layernorm.weight True\n",
      "model.layers.23.post_attention_layernorm.weight True\n",
      "model.layers.24.self_attn.q_proj.weight True\n",
      "model.layers.24.self_attn.k_proj.weight True\n",
      "model.layers.24.self_attn.v_proj.weight True\n",
      "model.layers.24.self_attn.o_proj.weight True\n",
      "model.layers.24.mlp.gate_proj.weight True\n",
      "model.layers.24.mlp.up_proj.weight True\n",
      "model.layers.24.mlp.down_proj.weight True\n",
      "model.layers.24.input_layernorm.weight True\n",
      "model.layers.24.post_attention_layernorm.weight True\n",
      "model.layers.25.self_attn.q_proj.weight True\n",
      "model.layers.25.self_attn.k_proj.weight True\n",
      "model.layers.25.self_attn.v_proj.weight True\n",
      "model.layers.25.self_attn.o_proj.weight True\n",
      "model.layers.25.mlp.gate_proj.weight True\n",
      "model.layers.25.mlp.up_proj.weight True\n",
      "model.layers.25.mlp.down_proj.weight True\n",
      "model.layers.25.input_layernorm.weight True\n",
      "model.layers.25.post_attention_layernorm.weight True\n",
      "model.layers.26.self_attn.q_proj.weight True\n",
      "model.layers.26.self_attn.k_proj.weight True\n",
      "model.layers.26.self_attn.v_proj.weight True\n",
      "model.layers.26.self_attn.o_proj.weight True\n",
      "model.layers.26.mlp.gate_proj.weight True\n",
      "model.layers.26.mlp.up_proj.weight True\n",
      "model.layers.26.mlp.down_proj.weight True\n",
      "model.layers.26.input_layernorm.weight True\n",
      "model.layers.26.post_attention_layernorm.weight True\n",
      "model.layers.27.self_attn.q_proj.weight True\n",
      "model.layers.27.self_attn.k_proj.weight True\n",
      "model.layers.27.self_attn.v_proj.weight True\n",
      "model.layers.27.self_attn.o_proj.weight True\n",
      "model.layers.27.mlp.gate_proj.weight True\n",
      "model.layers.27.mlp.up_proj.weight True\n",
      "model.layers.27.mlp.down_proj.weight True\n",
      "model.layers.27.input_layernorm.weight True\n",
      "model.layers.27.post_attention_layernorm.weight True\n",
      "model.layers.28.self_attn.q_proj.weight True\n",
      "model.layers.28.self_attn.k_proj.weight True\n",
      "model.layers.28.self_attn.v_proj.weight True\n",
      "model.layers.28.self_attn.o_proj.weight True\n",
      "model.layers.28.mlp.gate_proj.weight True\n",
      "model.layers.28.mlp.up_proj.weight True\n",
      "model.layers.28.mlp.down_proj.weight True\n",
      "model.layers.28.input_layernorm.weight True\n",
      "model.layers.28.post_attention_layernorm.weight True\n",
      "model.layers.29.self_attn.q_proj.weight True\n",
      "model.layers.29.self_attn.k_proj.weight True\n",
      "model.layers.29.self_attn.v_proj.weight True\n",
      "model.layers.29.self_attn.o_proj.weight True\n",
      "model.layers.29.mlp.gate_proj.weight True\n",
      "model.layers.29.mlp.up_proj.weight True\n",
      "model.layers.29.mlp.down_proj.weight True\n",
      "model.layers.29.input_layernorm.weight True\n",
      "model.layers.29.post_attention_layernorm.weight True\n",
      "model.layers.30.self_attn.q_proj.weight True\n",
      "model.layers.30.self_attn.k_proj.weight True\n",
      "model.layers.30.self_attn.v_proj.weight True\n",
      "model.layers.30.self_attn.o_proj.weight True\n",
      "model.layers.30.mlp.gate_proj.weight True\n",
      "model.layers.30.mlp.up_proj.weight True\n",
      "model.layers.30.mlp.down_proj.weight True\n",
      "model.layers.30.input_layernorm.weight True\n",
      "model.layers.30.post_attention_layernorm.weight True\n",
      "model.layers.31.self_attn.q_proj.weight True\n",
      "model.layers.31.self_attn.k_proj.weight True\n",
      "model.layers.31.self_attn.v_proj.weight True\n",
      "model.layers.31.self_attn.o_proj.weight True\n",
      "model.layers.31.mlp.gate_proj.weight True\n",
      "model.layers.31.mlp.up_proj.weight True\n",
      "model.layers.31.mlp.down_proj.weight True\n",
      "model.layers.31.input_layernorm.weight True\n",
      "model.layers.31.post_attention_layernorm.weight True\n",
      "model.norm.weight True\n",
      "lm_head.weight True\n",
      "Via relative path\n",
      "retrieval: evaluating model TxPLM on dataset protein_drugbank_drug_carrier , num_targets=18174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [01:46<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading cached target embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/vast-storage/scratch/vast/kellislab/rcalef/sandbox/repos/TxPLM/txplm/evaluate/framework/retrieval.py:182: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  F_max = 2 * precision * recall  / (precision + recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieval: evaluating model TxPLM on dataset protein_drugbank_drug_carrier_subset , num_targets=100\n",
      "WARNING: in dataset protein_drugbank_drug_carrier, ignoring a relation due to target not being found (id=557). In the case of using a specified subset of targets, this is expected, otherwise may want to check that all target IDs in dataset are in ALL_PROTEINS_FILE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [01:41<00:00,  3.27s/it]\n",
      "/net/vast-storage/scratch/vast/kellislab/rcalef/sandbox/repos/TxPLM/txplm/evaluate/framework/retrieval.py:182: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  F_max = 2 * precision * recall  / (precision + recall)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading cached target embeddings\n",
      "retrieval: evaluating model TxPLM on dataset protein_drugbank_drug_transporter , num_targets=18174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [06:20<00:00,  3.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading cached target embeddings\n"
     ]
    }
   ],
   "source": [
    "metrics = run_evaluation(eval_args, data_args, model_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run via CLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run via CLI, navigate to `test_dir` and run the following command\n",
    "```\n",
    "#! /bin/bash\n",
    "\n",
    "# Perform any setup for your environment\n",
    "#  source ~/.bashrc \n",
    "#  conda activate txplm\n",
    "\n",
    "# Run script using path to TxPLM repo\n",
    "#  python /path/to/TxPLM/scripts/run_eval_framework.py --from_yaml ./tmp_eval_config.yml 2>&1 | tee log.txt\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch space for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from txplm.evaluate.eval_v2 import (\n",
    "    load_and_validate_model_args,\n",
    "    load_datasets_for_eval,\n",
    "    get_target_set,\n",
    "    get_retrieval_target_proteins_loader,\n",
    "    prep_for_retrieval_eval,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ModelArgs from TxPLM checkpoint: /om2/vast/kellislab/shared/PLM/model_outputs/pretrain/2024-01-17_12:36_LLAMA2_z2_all/checkpoint-87500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Received training datasets for task qa, will not be used for evaluation (check data config)\n",
      "Received training datasets for task caption, will not be used for evaluation (check data config)\n",
      "Received training datasets for task retrieval, will not be used for evaluation (check data config)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task: retrieval dataset: protein_drugbank_drug_carrier_subset N: 486\n"
     ]
    }
   ],
   "source": [
    "# Check if we want to override ModelArgs using a TxPLM checkpoint\n",
    "if eval_args.model_args_from_checkpoint != \"\":\n",
    "    checkpoint_dir = eval_args.model_args_from_checkpoint\n",
    "    print(f\"Loading ModelArgs from TxPLM checkpoint: {checkpoint_dir}\")\n",
    "    model_args = torch.load(os.path.join(checkpoint_dir, \"model_args.pt\"))\n",
    "\n",
    "# Parse model specifications.\n",
    "models = load_and_validate_model_args(eval_args.models_config_yml)\n",
    "\n",
    "# Load datasets\n",
    "datasets, collators, dataset_eval_args = load_datasets_for_eval(data_args, model_args)\n",
    "for task, train_datasets in datasets[\"train\"].items():\n",
    "    if len(train_datasets) != 0:\n",
    "        logger.warning(\n",
    "            f\"Received training datasets for task {task}, will not be used for evaluation (check data config)\"\n",
    "        )\n",
    "# Package datasets and collators into data loaders.\n",
    "data_loaders = {}\n",
    "datasets = datasets[\"testing\"]\n",
    "collators = collators[\"testing\"]\n",
    "for task, task_datasets in datasets.items():\n",
    "    task_loaders = {}\n",
    "    for dataset_key, dataset in task_datasets.items():\n",
    "        print(f\"task: {task} dataset: {dataset_key} N: {len(dataset)}\")\n",
    "        task_loaders[dataset_key] = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=eval_args.batch_size,\n",
    "            collate_fn=collators[task][dataset_key],\n",
    "            num_workers=eval_args.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "    data_loaders[task] = task_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = data_loaders[\"retrieval\"][\"protein_drugbank_drug_carrier_subset\"]\n",
    "this_dataset_eval_args = dataset_eval_args[\"protein_drugbank_drug_carrier_subset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = get_target_set(\n",
    "    data_loader.dataset,\n",
    "    this_dataset_eval_args,\n",
    "    eval_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: in dataset protein_drugbank_drug_carrier, ignoring a relation due to target not being found (id=557). In the case of using a specified subset of targets, this is expected, otherwise may want to check that all target IDs in dataset are in ALL_PROTEINS_FILE\n"
     ]
    }
   ],
   "source": [
    "target_loader = get_retrieval_target_proteins_loader(\n",
    "    targets,\n",
    "    eval_args.batch_size,\n",
    ")\n",
    "labels, query_order, target_order = prep_for_retrieval_eval(\n",
    "    data_loader.dataset,\n",
    "    targets,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = next(iter(target_loader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
