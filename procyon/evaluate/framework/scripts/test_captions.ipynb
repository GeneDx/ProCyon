{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/om2/user/rcalef/miniconda3/envs/txplm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-04-17 23:30:36.532817: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-17 23:30:36.614676: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-17 23:30:40.613645: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-17 23:30:45,091] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from logging import getLogger\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import HfArgumentParser\n",
    "\n",
    "\n",
    "from txplm.evaluate.framework.core import run_evaluation\n",
    "from txplm.evaluate.framework.args import EvalArgs\n",
    "\n",
    "\n",
    "from txplm.training.training_args_IT import (\n",
    "    DataArgs,\n",
    "    ModelArgs,\n",
    "    postprocess_args,\n",
    ")\n",
    "\n",
    "from txplm.data.data_utils import (\n",
    "    DATA_DIR,\n",
    "    HOME_DIR,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint_dir = \"/om2/vast/kellislab/shared/PLM/model_outputs/pretrain/2024-01-17_12:36_LLAMA2_z2_all/checkpoint-87500\"\n",
    "checkpoint_dir = \"/om2/vast/kellislab/shared/PLM/model_outputs/pretrain/2024-03-25_04:40_txllm_ALL_split_v1_R9/checkpoint-3440000-OM\"\n",
    "test_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset.to_pickle(os.path.join(test_dir, \"test_protein_subset.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output config files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_yml = f\"\"\"it_datasets:\n",
    "  testing:\n",
    "    # DrugBank:\n",
    "    - aaseq_type: protein\n",
    "      text_type: drugbank\n",
    "      relations: [drug_carrier]\n",
    "      tasks: [caption]\n",
    "      splits: [eval_zero_shot, eval_pt_ft]\"\"\"\n",
    "\n",
    "with open(os.path.join(test_dir, \"tmp_eval_dataset_config.yml\"), \"w\") as fh:\n",
    "    fh.write(data_yml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_yml = f\"\"\"models:\n",
    "    - model_name: TxPLM\n",
    "      args:\n",
    "        checkpoint_dir: {checkpoint_dir}\n",
    "    - model_name: UniformRandom\n",
    "      args:\n",
    "        sample_from: full_dataset\n",
    "    - model_name: WeightedRandom\n",
    "      args:\n",
    "        sample_from: split\"\"\"\n",
    "\n",
    "with open(os.path.join(test_dir, \"tmp_eval_model_config.yml\"), \"w\") as fh:\n",
    "    fh.write(models_yml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_config = f\"\"\"# --------------------------------------   EVAL ARGUMENTS   --------------------------------------\n",
    "# Data config\n",
    "it_data_config_yml: {os.path.join(test_dir, \"tmp_eval_dataset_config.yml\")}\n",
    "models_config_yml: {os.path.join(test_dir, \"tmp_eval_model_config.yml\")}\n",
    "\n",
    "retrieval_use_cached_target_embeddings: False\n",
    "retrieval_eval_all_proteins: False\n",
    "retrieval_top_k_vals: [10, 20, 100]\n",
    "retrieval_balanced_metrics_num_samples: 5\n",
    "retrieval_balanced_metrics_neg_per_pos: 10\n",
    "\n",
    "batch_size: 4\n",
    "\n",
    "qa_num_samples: 5\n",
    "\n",
    "filter_training_pairs: False\n",
    "model_args_from_checkpoint: {checkpoint_dir}\n",
    "data_args_from_checkpoint: {checkpoint_dir}\n",
    "\n",
    "output_dir: {test_dir}\n",
    "\n",
    "use_cached_results: False\n",
    "\n",
    "# --------------------------------------   DATA ARGUMENTS   --------------------------------------\n",
    "# General:\n",
    "use_caption: False\n",
    "\n",
    "# Splitting:\n",
    "go_split_method: \"sample_aware_ontology_go_centric\"\n",
    "val_split_type: \"pt_ft\"\n",
    "\n",
    "# Dataset-specific attributes:\n",
    "go_def_col: \"standard\"\n",
    "\n",
    "# Negative sampling:\n",
    "num_neg_samples_qa: 1\n",
    "negative_sampling_strategy_qa: 'aaseq_only'\n",
    "negative_sampling_strategy_retrieval: 'in_batch'\n",
    "# --------------------------------------  MODEL ARGUMENTS   --------------------------------------\n",
    "protein_encoder_num_params: '35m'\n",
    "freeze_protein_encoder: \"all\"\n",
    "use_aaseq_embeddings: False\n",
    "freeze_aaseq_embeddings: False\n",
    "\n",
    "# Text encoder:\n",
    "use_text_embeddings: False\n",
    "freeze_text_embeddings: False\n",
    "text_encoder_fname: \"biogpt\"\n",
    "max_text_len: 512\n",
    "#freeze_text_encoder: \"all\"\n",
    "\n",
    "# Modeling-specific:\n",
    "ret_token_access: \"last\"\n",
    "train_qa_full_lm: False\n",
    "train_retrieval_lm: False\n",
    "roll_num: 1\"\"\"\n",
    "\n",
    "with open(os.path.join(test_dir, \"tmp_eval_config.yml\"), \"w\") as fh:\n",
    "    fh.write(example_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = HfArgumentParser((EvalArgs, DataArgs, ModelArgs))\n",
    "# train_args, data_args, model_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "eval_args, data_args, model_args = parser.parse_yaml_file(\"./tmp_eval_config.yml\")\n",
    "_, data_args, model_args = postprocess_args(None, data_args, model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch space for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from txplm.evaluate.framework.utils import (\n",
    "    compare_and_warn_model_args,\n",
    "    load_and_validate_model_args,\n",
    "    load_datasets_for_eval,\n",
    "    move_inputs_to_device,\n",
    ")\n",
    "from txplm.evaluate.framework.retrieval import (\n",
    "    calc_retrieval_metrics,\n",
    "    get_retrieval_target_set,\n",
    "    get_retrieval_target_proteins_loader,\n",
    "    prep_for_retrieval_eval,\n",
    ")\n",
    "\n",
    "from txplm.training.training_args_IT import (\n",
    "    update_data_args_data_dir,\n",
    "    update_model_args_data_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from txplm.evaluate.eval_utils import precision_recall_topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ModelArgs from TxPLM checkpoint: /om2/vast/kellislab/shared/PLM/model_outputs/pretrain/2024-03-25_04:40_txllm_ALL_split_v1_R9/checkpoint-3440000-OM\n",
      "Loading DataArgs from TxPLM checkpoint: /om2/vast/kellislab/shared/PLM/model_outputs/pretrain/2024-03-25_04:40_txllm_ALL_split_v1_R9/checkpoint-3440000-OM\n",
      "updating data args DATA_DIR from /n/holystore01/LABS/mzitnik_lab/Lab/PLM/ -> /om2/vast/kellislab/shared/PLM/\n",
      "task: caption dataset: protein_drugbank_drug_carrier_eval_zero_shot N: 13\n",
      "task: caption dataset: protein_drugbank_drug_carrier_eval_pt_ft N: 11\n"
     ]
    }
   ],
   "source": [
    "# Check if we want to override ModelArgs using a TxPLM checkpoint\n",
    "if eval_args.model_args_from_checkpoint != \"\":\n",
    "    checkpoint_dir = eval_args.model_args_from_checkpoint\n",
    "    print(f\"Loading ModelArgs from TxPLM checkpoint: {checkpoint_dir}\")\n",
    "    model_args = torch.load(os.path.join(checkpoint_dir, \"model_args.pt\"))\n",
    "\n",
    "\n",
    "if eval_args.data_args_from_checkpoint != \"\":\n",
    "    checkpoint_dir = eval_args.data_args_from_checkpoint\n",
    "    print(f\"Loading DataArgs from TxPLM checkpoint: {checkpoint_dir}\")\n",
    "    loaded_data_args = torch.load(os.path.join(checkpoint_dir, \"data_args.pt\"))\n",
    "    update_data_args_data_dir(loaded_data_args)\n",
    "\n",
    "    # Prefer to use the data config specified in data_args passed into this function\n",
    "    # over one specified in the serialized data config.\n",
    "    if data_args.it_data_config_yml is not None:\n",
    "        loaded_data_args.it_data_config_yml = data_args.it_data_config_yml\n",
    "    data_args = loaded_data_args\n",
    "\n",
    "\n",
    "# Parse model specifications.\n",
    "models = load_and_validate_model_args(eval_args.models_config_yml)\n",
    "\n",
    "# Load datasets\n",
    "datasets, collators, dataset_eval_args = load_datasets_for_eval(\n",
    "    data_args,\n",
    "    model_args,\n",
    "    eval_args.separate_splits,\n",
    ")\n",
    "for task, train_datasets in datasets[\"train\"].items():\n",
    "    if len(train_datasets) != 0:\n",
    "        print(\n",
    "            f\"Received training datasets for task {task}, will not be used for evaluation (check data config)\"\n",
    "        )\n",
    "# Package datasets and collators into data loaders.\n",
    "data_loaders = {}\n",
    "datasets = datasets[\"testing\"]\n",
    "collators = collators[\"testing\"]\n",
    "for task, task_datasets in datasets.items():\n",
    "    task_loaders = {}\n",
    "    for dataset_key, dataset in task_datasets.items():\n",
    "        print(f\"task: {task} dataset: {dataset_key} N: {len(dataset)}\")\n",
    "        task_loaders[dataset_key] = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=eval_args.batch_size,\n",
    "            collate_fn=collators[task][dataset_key],\n",
    "            num_workers=eval_args.num_workers,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "        )\n",
    "    data_loaders[task] = task_loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = data_loaders[\"caption\"][\"protein_drugbank_drug_carrier_eval_zero_shot\"]\n",
    "this_dataset_eval_args = dataset_eval_args[\"protein_drugbank_drug_carrier_eval_zero_shot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from txplm.evaluate.framework.txplm import TxPLMCaptionEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating model args DATA_DIR from /n/holystore01/LABS/mzitnik_lab/Lab/PLM -> /om2/vast/kellislab/shared/PLM/\n",
      "updating stale DATA_DIR for model arg: go_embeddings_path\n",
      "updating stale DATA_DIR for model arg: pfam_embeddings_path\n",
      "updating stale DATA_DIR for model arg: drugbank_embeddings_path\n",
      "updating stale DATA_DIR for model arg: reactome_embeddings_path\n",
      "updating stale DATA_DIR for model arg: omim_embeddings_path\n",
      "updating stale DATA_DIR for model arg: ec_embeddings_path\n",
      "updating stale DATA_DIR for model arg: protein_seq_embeddings_path\n",
      "updating stale DATA_DIR for model arg: protein_struct_embeddings_path\n",
      "updating stale DATA_DIR for model arg: protein_embeddings_idmap_path\n",
      "updating stale DATA_DIR for model arg: drug_struct_embeddings_path\n",
      "updating stale DATA_DIR for model arg: domain_embeddings_path\n",
      "updating stale DATA_DIR for model arg: domain_embeddings_idmap_path\n",
      "updating stale DATA_DIR for model arg: mouse_ortholog_embeddings_path\n",
      "updating stale DATA_DIR for model arg: mouse_ortholog_embeddings_idmap_path\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [02:34<00:00, 77.25s/it] \n",
      "Using sep_token, but it is not set yet.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "embed_tokens.weight False\n",
      "layers.0.self_attn.k_proj.weight False\n",
      "layers.0.self_attn.k_proj.bias False\n",
      "layers.0.self_attn.v_proj.weight False\n",
      "layers.0.self_attn.v_proj.bias False\n",
      "layers.0.self_attn.q_proj.weight False\n",
      "layers.0.self_attn.q_proj.bias False\n",
      "layers.0.self_attn.q_proj_lora_d.weight True\n",
      "layers.0.self_attn.q_proj_lora_u.weight True\n",
      "layers.0.self_attn.v_proj_lora_d.weight True\n",
      "layers.0.self_attn.v_proj_lora_u.weight True\n",
      "layers.0.self_attn.out_proj.weight False\n",
      "layers.0.self_attn.out_proj.bias False\n",
      "layers.0.self_attn_layer_norm.weight False\n",
      "layers.0.self_attn_layer_norm.bias False\n",
      "layers.0.fc1.weight False\n",
      "layers.0.fc1.bias False\n",
      "layers.0.fc2.weight False\n",
      "layers.0.fc2.bias False\n",
      "layers.0.final_layer_norm.weight False\n",
      "layers.0.final_layer_norm.bias False\n",
      "layers.1.self_attn.k_proj.weight False\n",
      "layers.1.self_attn.k_proj.bias False\n",
      "layers.1.self_attn.v_proj.weight False\n",
      "layers.1.self_attn.v_proj.bias False\n",
      "layers.1.self_attn.q_proj.weight False\n",
      "layers.1.self_attn.q_proj.bias False\n",
      "layers.1.self_attn.q_proj_lora_d.weight True\n",
      "layers.1.self_attn.q_proj_lora_u.weight True\n",
      "layers.1.self_attn.v_proj_lora_d.weight True\n",
      "layers.1.self_attn.v_proj_lora_u.weight True\n",
      "layers.1.self_attn.out_proj.weight False\n",
      "layers.1.self_attn.out_proj.bias False\n",
      "layers.1.self_attn_layer_norm.weight False\n",
      "layers.1.self_attn_layer_norm.bias False\n",
      "layers.1.fc1.weight False\n",
      "layers.1.fc1.bias False\n",
      "layers.1.fc2.weight False\n",
      "layers.1.fc2.bias False\n",
      "layers.1.final_layer_norm.weight False\n",
      "layers.1.final_layer_norm.bias False\n",
      "layers.2.self_attn.k_proj.weight False\n",
      "layers.2.self_attn.k_proj.bias False\n",
      "layers.2.self_attn.v_proj.weight False\n",
      "layers.2.self_attn.v_proj.bias False\n",
      "layers.2.self_attn.q_proj.weight False\n",
      "layers.2.self_attn.q_proj.bias False\n",
      "layers.2.self_attn.q_proj_lora_d.weight True\n",
      "layers.2.self_attn.q_proj_lora_u.weight True\n",
      "layers.2.self_attn.v_proj_lora_d.weight True\n",
      "layers.2.self_attn.v_proj_lora_u.weight True\n",
      "layers.2.self_attn.out_proj.weight False\n",
      "layers.2.self_attn.out_proj.bias False\n",
      "layers.2.self_attn_layer_norm.weight False\n",
      "layers.2.self_attn_layer_norm.bias False\n",
      "layers.2.fc1.weight False\n",
      "layers.2.fc1.bias False\n",
      "layers.2.fc2.weight False\n",
      "layers.2.fc2.bias False\n",
      "layers.2.final_layer_norm.weight False\n",
      "layers.2.final_layer_norm.bias False\n",
      "layers.3.self_attn.k_proj.weight False\n",
      "layers.3.self_attn.k_proj.bias False\n",
      "layers.3.self_attn.v_proj.weight False\n",
      "layers.3.self_attn.v_proj.bias False\n",
      "layers.3.self_attn.q_proj.weight False\n",
      "layers.3.self_attn.q_proj.bias False\n",
      "layers.3.self_attn.q_proj_lora_d.weight True\n",
      "layers.3.self_attn.q_proj_lora_u.weight True\n",
      "layers.3.self_attn.v_proj_lora_d.weight True\n",
      "layers.3.self_attn.v_proj_lora_u.weight True\n",
      "layers.3.self_attn.out_proj.weight False\n",
      "layers.3.self_attn.out_proj.bias False\n",
      "layers.3.self_attn_layer_norm.weight False\n",
      "layers.3.self_attn_layer_norm.bias False\n",
      "layers.3.fc1.weight False\n",
      "layers.3.fc1.bias False\n",
      "layers.3.fc2.weight False\n",
      "layers.3.fc2.bias False\n",
      "layers.3.final_layer_norm.weight False\n",
      "layers.3.final_layer_norm.bias False\n",
      "layers.4.self_attn.k_proj.weight False\n",
      "layers.4.self_attn.k_proj.bias False\n",
      "layers.4.self_attn.v_proj.weight False\n",
      "layers.4.self_attn.v_proj.bias False\n",
      "layers.4.self_attn.q_proj.weight False\n",
      "layers.4.self_attn.q_proj.bias False\n",
      "layers.4.self_attn.q_proj_lora_d.weight True\n",
      "layers.4.self_attn.q_proj_lora_u.weight True\n",
      "layers.4.self_attn.v_proj_lora_d.weight True\n",
      "layers.4.self_attn.v_proj_lora_u.weight True\n",
      "layers.4.self_attn.out_proj.weight False\n",
      "layers.4.self_attn.out_proj.bias False\n",
      "layers.4.self_attn_layer_norm.weight False\n",
      "layers.4.self_attn_layer_norm.bias False\n",
      "layers.4.fc1.weight False\n",
      "layers.4.fc1.bias False\n",
      "layers.4.fc2.weight False\n",
      "layers.4.fc2.bias False\n",
      "layers.4.final_layer_norm.weight False\n",
      "layers.4.final_layer_norm.bias False\n",
      "layers.5.self_attn.k_proj.weight False\n",
      "layers.5.self_attn.k_proj.bias False\n",
      "layers.5.self_attn.v_proj.weight False\n",
      "layers.5.self_attn.v_proj.bias False\n",
      "layers.5.self_attn.q_proj.weight False\n",
      "layers.5.self_attn.q_proj.bias False\n",
      "layers.5.self_attn.q_proj_lora_d.weight True\n",
      "layers.5.self_attn.q_proj_lora_u.weight True\n",
      "layers.5.self_attn.v_proj_lora_d.weight True\n",
      "layers.5.self_attn.v_proj_lora_u.weight True\n",
      "layers.5.self_attn.out_proj.weight False\n",
      "layers.5.self_attn.out_proj.bias False\n",
      "layers.5.self_attn_layer_norm.weight False\n",
      "layers.5.self_attn_layer_norm.bias False\n",
      "layers.5.fc1.weight False\n",
      "layers.5.fc1.bias False\n",
      "layers.5.fc2.weight False\n",
      "layers.5.fc2.bias False\n",
      "layers.5.final_layer_norm.weight False\n",
      "layers.5.final_layer_norm.bias False\n",
      "layers.6.self_attn.k_proj.weight False\n",
      "layers.6.self_attn.k_proj.bias False\n",
      "layers.6.self_attn.v_proj.weight False\n",
      "layers.6.self_attn.v_proj.bias False\n",
      "layers.6.self_attn.q_proj.weight False\n",
      "layers.6.self_attn.q_proj.bias False\n",
      "layers.6.self_attn.q_proj_lora_d.weight True\n",
      "layers.6.self_attn.q_proj_lora_u.weight True\n",
      "layers.6.self_attn.v_proj_lora_d.weight True\n",
      "layers.6.self_attn.v_proj_lora_u.weight True\n",
      "layers.6.self_attn.out_proj.weight False\n",
      "layers.6.self_attn.out_proj.bias False\n",
      "layers.6.self_attn_layer_norm.weight False\n",
      "layers.6.self_attn_layer_norm.bias False\n",
      "layers.6.fc1.weight False\n",
      "layers.6.fc1.bias False\n",
      "layers.6.fc2.weight False\n",
      "layers.6.fc2.bias False\n",
      "layers.6.final_layer_norm.weight False\n",
      "layers.6.final_layer_norm.bias False\n",
      "layers.7.self_attn.k_proj.weight False\n",
      "layers.7.self_attn.k_proj.bias False\n",
      "layers.7.self_attn.v_proj.weight False\n",
      "layers.7.self_attn.v_proj.bias False\n",
      "layers.7.self_attn.q_proj.weight False\n",
      "layers.7.self_attn.q_proj.bias False\n",
      "layers.7.self_attn.q_proj_lora_d.weight True\n",
      "layers.7.self_attn.q_proj_lora_u.weight True\n",
      "layers.7.self_attn.v_proj_lora_d.weight True\n",
      "layers.7.self_attn.v_proj_lora_u.weight True\n",
      "layers.7.self_attn.out_proj.weight False\n",
      "layers.7.self_attn.out_proj.bias False\n",
      "layers.7.self_attn_layer_norm.weight False\n",
      "layers.7.self_attn_layer_norm.bias False\n",
      "layers.7.fc1.weight False\n",
      "layers.7.fc1.bias False\n",
      "layers.7.fc2.weight False\n",
      "layers.7.fc2.bias False\n",
      "layers.7.final_layer_norm.weight False\n",
      "layers.7.final_layer_norm.bias False\n",
      "layers.8.self_attn.k_proj.weight False\n",
      "layers.8.self_attn.k_proj.bias False\n",
      "layers.8.self_attn.v_proj.weight False\n",
      "layers.8.self_attn.v_proj.bias False\n",
      "layers.8.self_attn.q_proj.weight False\n",
      "layers.8.self_attn.q_proj.bias False\n",
      "layers.8.self_attn.q_proj_lora_d.weight True\n",
      "layers.8.self_attn.q_proj_lora_u.weight True\n",
      "layers.8.self_attn.v_proj_lora_d.weight True\n",
      "layers.8.self_attn.v_proj_lora_u.weight True\n",
      "layers.8.self_attn.out_proj.weight False\n",
      "layers.8.self_attn.out_proj.bias False\n",
      "layers.8.self_attn_layer_norm.weight False\n",
      "layers.8.self_attn_layer_norm.bias False\n",
      "layers.8.fc1.weight False\n",
      "layers.8.fc1.bias False\n",
      "layers.8.fc2.weight False\n",
      "layers.8.fc2.bias False\n",
      "layers.8.final_layer_norm.weight False\n",
      "layers.8.final_layer_norm.bias False\n",
      "layers.9.self_attn.k_proj.weight False\n",
      "layers.9.self_attn.k_proj.bias False\n",
      "layers.9.self_attn.v_proj.weight False\n",
      "layers.9.self_attn.v_proj.bias False\n",
      "layers.9.self_attn.q_proj.weight False\n",
      "layers.9.self_attn.q_proj.bias False\n",
      "layers.9.self_attn.q_proj_lora_d.weight True\n",
      "layers.9.self_attn.q_proj_lora_u.weight True\n",
      "layers.9.self_attn.v_proj_lora_d.weight True\n",
      "layers.9.self_attn.v_proj_lora_u.weight True\n",
      "layers.9.self_attn.out_proj.weight False\n",
      "layers.9.self_attn.out_proj.bias False\n",
      "layers.9.self_attn_layer_norm.weight False\n",
      "layers.9.self_attn_layer_norm.bias False\n",
      "layers.9.fc1.weight False\n",
      "layers.9.fc1.bias False\n",
      "layers.9.fc2.weight False\n",
      "layers.9.fc2.bias False\n",
      "layers.9.final_layer_norm.weight False\n",
      "layers.9.final_layer_norm.bias False\n",
      "layers.10.self_attn.k_proj.weight False\n",
      "layers.10.self_attn.k_proj.bias False\n",
      "layers.10.self_attn.v_proj.weight False\n",
      "layers.10.self_attn.v_proj.bias False\n",
      "layers.10.self_attn.q_proj.weight False\n",
      "layers.10.self_attn.q_proj.bias False\n",
      "layers.10.self_attn.q_proj_lora_d.weight True\n",
      "layers.10.self_attn.q_proj_lora_u.weight True\n",
      "layers.10.self_attn.v_proj_lora_d.weight True\n",
      "layers.10.self_attn.v_proj_lora_u.weight True\n",
      "layers.10.self_attn.out_proj.weight False\n",
      "layers.10.self_attn.out_proj.bias False\n",
      "layers.10.self_attn_layer_norm.weight False\n",
      "layers.10.self_attn_layer_norm.bias False\n",
      "layers.10.fc1.weight False\n",
      "layers.10.fc1.bias False\n",
      "layers.10.fc2.weight False\n",
      "layers.10.fc2.bias False\n",
      "layers.10.final_layer_norm.weight False\n",
      "layers.10.final_layer_norm.bias False\n",
      "layers.11.self_attn.k_proj.weight False\n",
      "layers.11.self_attn.k_proj.bias False\n",
      "layers.11.self_attn.v_proj.weight False\n",
      "layers.11.self_attn.v_proj.bias False\n",
      "layers.11.self_attn.q_proj.weight False\n",
      "layers.11.self_attn.q_proj.bias False\n",
      "layers.11.self_attn.q_proj_lora_d.weight True\n",
      "layers.11.self_attn.q_proj_lora_u.weight True\n",
      "layers.11.self_attn.v_proj_lora_d.weight True\n",
      "layers.11.self_attn.v_proj_lora_u.weight True\n",
      "layers.11.self_attn.out_proj.weight False\n",
      "layers.11.self_attn.out_proj.bias False\n",
      "layers.11.self_attn_layer_norm.weight False\n",
      "layers.11.self_attn_layer_norm.bias False\n",
      "layers.11.fc1.weight False\n",
      "layers.11.fc1.bias False\n",
      "layers.11.fc2.weight False\n",
      "layers.11.fc2.bias False\n",
      "layers.11.final_layer_norm.weight False\n",
      "layers.11.final_layer_norm.bias False\n",
      "layers.12.self_attn.k_proj.weight False\n",
      "layers.12.self_attn.k_proj.bias False\n",
      "layers.12.self_attn.v_proj.weight False\n",
      "layers.12.self_attn.v_proj.bias False\n",
      "layers.12.self_attn.q_proj.weight False\n",
      "layers.12.self_attn.q_proj.bias False\n",
      "layers.12.self_attn.q_proj_lora_d.weight True\n",
      "layers.12.self_attn.q_proj_lora_u.weight True\n",
      "layers.12.self_attn.v_proj_lora_d.weight True\n",
      "layers.12.self_attn.v_proj_lora_u.weight True\n",
      "layers.12.self_attn.out_proj.weight False\n",
      "layers.12.self_attn.out_proj.bias False\n",
      "layers.12.self_attn_layer_norm.weight False\n",
      "layers.12.self_attn_layer_norm.bias False\n",
      "layers.12.fc1.weight False\n",
      "layers.12.fc1.bias False\n",
      "layers.12.fc2.weight False\n",
      "layers.12.fc2.bias False\n",
      "layers.12.final_layer_norm.weight False\n",
      "layers.12.final_layer_norm.bias False\n",
      "layers.13.self_attn.k_proj.weight False\n",
      "layers.13.self_attn.k_proj.bias False\n",
      "layers.13.self_attn.v_proj.weight False\n",
      "layers.13.self_attn.v_proj.bias False\n",
      "layers.13.self_attn.q_proj.weight False\n",
      "layers.13.self_attn.q_proj.bias False\n",
      "layers.13.self_attn.q_proj_lora_d.weight True\n",
      "layers.13.self_attn.q_proj_lora_u.weight True\n",
      "layers.13.self_attn.v_proj_lora_d.weight True\n",
      "layers.13.self_attn.v_proj_lora_u.weight True\n",
      "layers.13.self_attn.out_proj.weight False\n",
      "layers.13.self_attn.out_proj.bias False\n",
      "layers.13.self_attn_layer_norm.weight False\n",
      "layers.13.self_attn_layer_norm.bias False\n",
      "layers.13.fc1.weight False\n",
      "layers.13.fc1.bias False\n",
      "layers.13.fc2.weight False\n",
      "layers.13.fc2.bias False\n",
      "layers.13.final_layer_norm.weight False\n",
      "layers.13.final_layer_norm.bias False\n",
      "layers.14.self_attn.k_proj.weight False\n",
      "layers.14.self_attn.k_proj.bias False\n",
      "layers.14.self_attn.v_proj.weight False\n",
      "layers.14.self_attn.v_proj.bias False\n",
      "layers.14.self_attn.q_proj.weight False\n",
      "layers.14.self_attn.q_proj.bias False\n",
      "layers.14.self_attn.q_proj_lora_d.weight True\n",
      "layers.14.self_attn.q_proj_lora_u.weight True\n",
      "layers.14.self_attn.v_proj_lora_d.weight True\n",
      "layers.14.self_attn.v_proj_lora_u.weight True\n",
      "layers.14.self_attn.out_proj.weight False\n",
      "layers.14.self_attn.out_proj.bias False\n",
      "layers.14.self_attn_layer_norm.weight False\n",
      "layers.14.self_attn_layer_norm.bias False\n",
      "layers.14.fc1.weight False\n",
      "layers.14.fc1.bias False\n",
      "layers.14.fc2.weight False\n",
      "layers.14.fc2.bias False\n",
      "layers.14.final_layer_norm.weight False\n",
      "layers.14.final_layer_norm.bias False\n",
      "layers.15.self_attn.k_proj.weight False\n",
      "layers.15.self_attn.k_proj.bias False\n",
      "layers.15.self_attn.v_proj.weight False\n",
      "layers.15.self_attn.v_proj.bias False\n",
      "layers.15.self_attn.q_proj.weight False\n",
      "layers.15.self_attn.q_proj.bias False\n",
      "layers.15.self_attn.q_proj_lora_d.weight True\n",
      "layers.15.self_attn.q_proj_lora_u.weight True\n",
      "layers.15.self_attn.v_proj_lora_d.weight True\n",
      "layers.15.self_attn.v_proj_lora_u.weight True\n",
      "layers.15.self_attn.out_proj.weight False\n",
      "layers.15.self_attn.out_proj.bias False\n",
      "layers.15.self_attn_layer_norm.weight False\n",
      "layers.15.self_attn_layer_norm.bias False\n",
      "layers.15.fc1.weight False\n",
      "layers.15.fc1.bias False\n",
      "layers.15.fc2.weight False\n",
      "layers.15.fc2.bias False\n",
      "layers.15.final_layer_norm.weight False\n",
      "layers.15.final_layer_norm.bias False\n",
      "layers.16.self_attn.k_proj.weight False\n",
      "layers.16.self_attn.k_proj.bias False\n",
      "layers.16.self_attn.v_proj.weight False\n",
      "layers.16.self_attn.v_proj.bias False\n",
      "layers.16.self_attn.q_proj.weight False\n",
      "layers.16.self_attn.q_proj.bias False\n",
      "layers.16.self_attn.q_proj_lora_d.weight True\n",
      "layers.16.self_attn.q_proj_lora_u.weight True\n",
      "layers.16.self_attn.v_proj_lora_d.weight True\n",
      "layers.16.self_attn.v_proj_lora_u.weight True\n",
      "layers.16.self_attn.out_proj.weight False\n",
      "layers.16.self_attn.out_proj.bias False\n",
      "layers.16.self_attn_layer_norm.weight False\n",
      "layers.16.self_attn_layer_norm.bias False\n",
      "layers.16.fc1.weight False\n",
      "layers.16.fc1.bias False\n",
      "layers.16.fc2.weight False\n",
      "layers.16.fc2.bias False\n",
      "layers.16.final_layer_norm.weight False\n",
      "layers.16.final_layer_norm.bias False\n",
      "layers.17.self_attn.k_proj.weight False\n",
      "layers.17.self_attn.k_proj.bias False\n",
      "layers.17.self_attn.v_proj.weight False\n",
      "layers.17.self_attn.v_proj.bias False\n",
      "layers.17.self_attn.q_proj.weight False\n",
      "layers.17.self_attn.q_proj.bias False\n",
      "layers.17.self_attn.q_proj_lora_d.weight True\n",
      "layers.17.self_attn.q_proj_lora_u.weight True\n",
      "layers.17.self_attn.v_proj_lora_d.weight True\n",
      "layers.17.self_attn.v_proj_lora_u.weight True\n",
      "layers.17.self_attn.out_proj.weight False\n",
      "layers.17.self_attn.out_proj.bias False\n",
      "layers.17.self_attn_layer_norm.weight False\n",
      "layers.17.self_attn_layer_norm.bias False\n",
      "layers.17.fc1.weight False\n",
      "layers.17.fc1.bias False\n",
      "layers.17.fc2.weight False\n",
      "layers.17.fc2.bias False\n",
      "layers.17.final_layer_norm.weight False\n",
      "layers.17.final_layer_norm.bias False\n",
      "layers.18.self_attn.k_proj.weight False\n",
      "layers.18.self_attn.k_proj.bias False\n",
      "layers.18.self_attn.v_proj.weight False\n",
      "layers.18.self_attn.v_proj.bias False\n",
      "layers.18.self_attn.q_proj.weight False\n",
      "layers.18.self_attn.q_proj.bias False\n",
      "layers.18.self_attn.q_proj_lora_d.weight True\n",
      "layers.18.self_attn.q_proj_lora_u.weight True\n",
      "layers.18.self_attn.v_proj_lora_d.weight True\n",
      "layers.18.self_attn.v_proj_lora_u.weight True\n",
      "layers.18.self_attn.out_proj.weight False\n",
      "layers.18.self_attn.out_proj.bias False\n",
      "layers.18.self_attn_layer_norm.weight False\n",
      "layers.18.self_attn_layer_norm.bias False\n",
      "layers.18.fc1.weight False\n",
      "layers.18.fc1.bias False\n",
      "layers.18.fc2.weight False\n",
      "layers.18.fc2.bias False\n",
      "layers.18.final_layer_norm.weight False\n",
      "layers.18.final_layer_norm.bias False\n",
      "layers.19.self_attn.k_proj.weight False\n",
      "layers.19.self_attn.k_proj.bias False\n",
      "layers.19.self_attn.v_proj.weight False\n",
      "layers.19.self_attn.v_proj.bias False\n",
      "layers.19.self_attn.q_proj.weight False\n",
      "layers.19.self_attn.q_proj.bias False\n",
      "layers.19.self_attn.q_proj_lora_d.weight True\n",
      "layers.19.self_attn.q_proj_lora_u.weight True\n",
      "layers.19.self_attn.v_proj_lora_d.weight True\n",
      "layers.19.self_attn.v_proj_lora_u.weight True\n",
      "layers.19.self_attn.out_proj.weight False\n",
      "layers.19.self_attn.out_proj.bias False\n",
      "layers.19.self_attn_layer_norm.weight False\n",
      "layers.19.self_attn_layer_norm.bias False\n",
      "layers.19.fc1.weight False\n",
      "layers.19.fc1.bias False\n",
      "layers.19.fc2.weight False\n",
      "layers.19.fc2.bias False\n",
      "layers.19.final_layer_norm.weight False\n",
      "layers.19.final_layer_norm.bias False\n",
      "layers.20.self_attn.k_proj.weight False\n",
      "layers.20.self_attn.k_proj.bias False\n",
      "layers.20.self_attn.v_proj.weight False\n",
      "layers.20.self_attn.v_proj.bias False\n",
      "layers.20.self_attn.q_proj.weight False\n",
      "layers.20.self_attn.q_proj.bias False\n",
      "layers.20.self_attn.q_proj_lora_d.weight True\n",
      "layers.20.self_attn.q_proj_lora_u.weight True\n",
      "layers.20.self_attn.v_proj_lora_d.weight True\n",
      "layers.20.self_attn.v_proj_lora_u.weight True\n",
      "layers.20.self_attn.out_proj.weight False\n",
      "layers.20.self_attn.out_proj.bias False\n",
      "layers.20.self_attn_layer_norm.weight False\n",
      "layers.20.self_attn_layer_norm.bias False\n",
      "layers.20.fc1.weight False\n",
      "layers.20.fc1.bias False\n",
      "layers.20.fc2.weight False\n",
      "layers.20.fc2.bias False\n",
      "layers.20.final_layer_norm.weight False\n",
      "layers.20.final_layer_norm.bias False\n",
      "layers.21.self_attn.k_proj.weight False\n",
      "layers.21.self_attn.k_proj.bias False\n",
      "layers.21.self_attn.v_proj.weight False\n",
      "layers.21.self_attn.v_proj.bias False\n",
      "layers.21.self_attn.q_proj.weight False\n",
      "layers.21.self_attn.q_proj.bias False\n",
      "layers.21.self_attn.q_proj_lora_d.weight True\n",
      "layers.21.self_attn.q_proj_lora_u.weight True\n",
      "layers.21.self_attn.v_proj_lora_d.weight True\n",
      "layers.21.self_attn.v_proj_lora_u.weight True\n",
      "layers.21.self_attn.out_proj.weight False\n",
      "layers.21.self_attn.out_proj.bias False\n",
      "layers.21.self_attn_layer_norm.weight False\n",
      "layers.21.self_attn_layer_norm.bias False\n",
      "layers.21.fc1.weight False\n",
      "layers.21.fc1.bias False\n",
      "layers.21.fc2.weight False\n",
      "layers.21.fc2.bias False\n",
      "layers.21.final_layer_norm.weight False\n",
      "layers.21.final_layer_norm.bias False\n",
      "layers.22.self_attn.k_proj.weight False\n",
      "layers.22.self_attn.k_proj.bias False\n",
      "layers.22.self_attn.v_proj.weight False\n",
      "layers.22.self_attn.v_proj.bias False\n",
      "layers.22.self_attn.q_proj.weight False\n",
      "layers.22.self_attn.q_proj.bias False\n",
      "layers.22.self_attn.q_proj_lora_d.weight True\n",
      "layers.22.self_attn.q_proj_lora_u.weight True\n",
      "layers.22.self_attn.v_proj_lora_d.weight True\n",
      "layers.22.self_attn.v_proj_lora_u.weight True\n",
      "layers.22.self_attn.out_proj.weight False\n",
      "layers.22.self_attn.out_proj.bias False\n",
      "layers.22.self_attn_layer_norm.weight False\n",
      "layers.22.self_attn_layer_norm.bias False\n",
      "layers.22.fc1.weight False\n",
      "layers.22.fc1.bias False\n",
      "layers.22.fc2.weight False\n",
      "layers.22.fc2.bias False\n",
      "layers.22.final_layer_norm.weight False\n",
      "layers.22.final_layer_norm.bias False\n",
      "layers.23.self_attn.k_proj.weight False\n",
      "layers.23.self_attn.k_proj.bias False\n",
      "layers.23.self_attn.v_proj.weight False\n",
      "layers.23.self_attn.v_proj.bias False\n",
      "layers.23.self_attn.q_proj.weight False\n",
      "layers.23.self_attn.q_proj.bias False\n",
      "layers.23.self_attn.q_proj_lora_d.weight True\n",
      "layers.23.self_attn.q_proj_lora_u.weight True\n",
      "layers.23.self_attn.v_proj_lora_d.weight True\n",
      "layers.23.self_attn.v_proj_lora_u.weight True\n",
      "layers.23.self_attn.out_proj.weight False\n",
      "layers.23.self_attn.out_proj.bias False\n",
      "layers.23.self_attn_layer_norm.weight False\n",
      "layers.23.self_attn_layer_norm.bias False\n",
      "layers.23.fc1.weight False\n",
      "layers.23.fc1.bias False\n",
      "layers.23.fc2.weight False\n",
      "layers.23.fc2.bias False\n",
      "layers.23.final_layer_norm.weight False\n",
      "layers.23.final_layer_norm.bias False\n",
      "layers.24.self_attn.k_proj.weight False\n",
      "layers.24.self_attn.k_proj.bias False\n",
      "layers.24.self_attn.v_proj.weight False\n",
      "layers.24.self_attn.v_proj.bias False\n",
      "layers.24.self_attn.q_proj.weight False\n",
      "layers.24.self_attn.q_proj.bias False\n",
      "layers.24.self_attn.q_proj_lora_d.weight True\n",
      "layers.24.self_attn.q_proj_lora_u.weight True\n",
      "layers.24.self_attn.v_proj_lora_d.weight True\n",
      "layers.24.self_attn.v_proj_lora_u.weight True\n",
      "layers.24.self_attn.out_proj.weight False\n",
      "layers.24.self_attn.out_proj.bias False\n",
      "layers.24.self_attn_layer_norm.weight False\n",
      "layers.24.self_attn_layer_norm.bias False\n",
      "layers.24.fc1.weight False\n",
      "layers.24.fc1.bias False\n",
      "layers.24.fc2.weight False\n",
      "layers.24.fc2.bias False\n",
      "layers.24.final_layer_norm.weight False\n",
      "layers.24.final_layer_norm.bias False\n",
      "layers.25.self_attn.k_proj.weight False\n",
      "layers.25.self_attn.k_proj.bias False\n",
      "layers.25.self_attn.v_proj.weight False\n",
      "layers.25.self_attn.v_proj.bias False\n",
      "layers.25.self_attn.q_proj.weight False\n",
      "layers.25.self_attn.q_proj.bias False\n",
      "layers.25.self_attn.q_proj_lora_d.weight True\n",
      "layers.25.self_attn.q_proj_lora_u.weight True\n",
      "layers.25.self_attn.v_proj_lora_d.weight True\n",
      "layers.25.self_attn.v_proj_lora_u.weight True\n",
      "layers.25.self_attn.out_proj.weight False\n",
      "layers.25.self_attn.out_proj.bias False\n",
      "layers.25.self_attn_layer_norm.weight False\n",
      "layers.25.self_attn_layer_norm.bias False\n",
      "layers.25.fc1.weight False\n",
      "layers.25.fc1.bias False\n",
      "layers.25.fc2.weight False\n",
      "layers.25.fc2.bias False\n",
      "layers.25.final_layer_norm.weight False\n",
      "layers.25.final_layer_norm.bias False\n",
      "layers.26.self_attn.k_proj.weight False\n",
      "layers.26.self_attn.k_proj.bias False\n",
      "layers.26.self_attn.v_proj.weight False\n",
      "layers.26.self_attn.v_proj.bias False\n",
      "layers.26.self_attn.q_proj.weight False\n",
      "layers.26.self_attn.q_proj.bias False\n",
      "layers.26.self_attn.q_proj_lora_d.weight True\n",
      "layers.26.self_attn.q_proj_lora_u.weight True\n",
      "layers.26.self_attn.v_proj_lora_d.weight True\n",
      "layers.26.self_attn.v_proj_lora_u.weight True\n",
      "layers.26.self_attn.out_proj.weight False\n",
      "layers.26.self_attn.out_proj.bias False\n",
      "layers.26.self_attn_layer_norm.weight False\n",
      "layers.26.self_attn_layer_norm.bias False\n",
      "layers.26.fc1.weight False\n",
      "layers.26.fc1.bias False\n",
      "layers.26.fc2.weight False\n",
      "layers.26.fc2.bias False\n",
      "layers.26.final_layer_norm.weight False\n",
      "layers.26.final_layer_norm.bias False\n",
      "layers.27.self_attn.k_proj.weight False\n",
      "layers.27.self_attn.k_proj.bias False\n",
      "layers.27.self_attn.v_proj.weight False\n",
      "layers.27.self_attn.v_proj.bias False\n",
      "layers.27.self_attn.q_proj.weight False\n",
      "layers.27.self_attn.q_proj.bias False\n",
      "layers.27.self_attn.q_proj_lora_d.weight True\n",
      "layers.27.self_attn.q_proj_lora_u.weight True\n",
      "layers.27.self_attn.v_proj_lora_d.weight True\n",
      "layers.27.self_attn.v_proj_lora_u.weight True\n",
      "layers.27.self_attn.out_proj.weight False\n",
      "layers.27.self_attn.out_proj.bias False\n",
      "layers.27.self_attn_layer_norm.weight False\n",
      "layers.27.self_attn_layer_norm.bias False\n",
      "layers.27.fc1.weight False\n",
      "layers.27.fc1.bias False\n",
      "layers.27.fc2.weight False\n",
      "layers.27.fc2.bias False\n",
      "layers.27.final_layer_norm.weight False\n",
      "layers.27.final_layer_norm.bias False\n",
      "layers.28.self_attn.k_proj.weight False\n",
      "layers.28.self_attn.k_proj.bias False\n",
      "layers.28.self_attn.v_proj.weight False\n",
      "layers.28.self_attn.v_proj.bias False\n",
      "layers.28.self_attn.q_proj.weight False\n",
      "layers.28.self_attn.q_proj.bias False\n",
      "layers.28.self_attn.q_proj_lora_d.weight True\n",
      "layers.28.self_attn.q_proj_lora_u.weight True\n",
      "layers.28.self_attn.v_proj_lora_d.weight True\n",
      "layers.28.self_attn.v_proj_lora_u.weight True\n",
      "layers.28.self_attn.out_proj.weight False\n",
      "layers.28.self_attn.out_proj.bias False\n",
      "layers.28.self_attn_layer_norm.weight False\n",
      "layers.28.self_attn_layer_norm.bias False\n",
      "layers.28.fc1.weight False\n",
      "layers.28.fc1.bias False\n",
      "layers.28.fc2.weight False\n",
      "layers.28.fc2.bias False\n",
      "layers.28.final_layer_norm.weight False\n",
      "layers.28.final_layer_norm.bias False\n",
      "layers.29.self_attn.k_proj.weight False\n",
      "layers.29.self_attn.k_proj.bias False\n",
      "layers.29.self_attn.v_proj.weight False\n",
      "layers.29.self_attn.v_proj.bias False\n",
      "layers.29.self_attn.q_proj.weight False\n",
      "layers.29.self_attn.q_proj.bias False\n",
      "layers.29.self_attn.q_proj_lora_d.weight True\n",
      "layers.29.self_attn.q_proj_lora_u.weight True\n",
      "layers.29.self_attn.v_proj_lora_d.weight True\n",
      "layers.29.self_attn.v_proj_lora_u.weight True\n",
      "layers.29.self_attn.out_proj.weight False\n",
      "layers.29.self_attn.out_proj.bias False\n",
      "layers.29.self_attn_layer_norm.weight False\n",
      "layers.29.self_attn_layer_norm.bias False\n",
      "layers.29.fc1.weight False\n",
      "layers.29.fc1.bias False\n",
      "layers.29.fc2.weight False\n",
      "layers.29.fc2.bias False\n",
      "layers.29.final_layer_norm.weight False\n",
      "layers.29.final_layer_norm.bias False\n",
      "layers.30.self_attn.k_proj.weight False\n",
      "layers.30.self_attn.k_proj.bias False\n",
      "layers.30.self_attn.v_proj.weight False\n",
      "layers.30.self_attn.v_proj.bias False\n",
      "layers.30.self_attn.q_proj.weight False\n",
      "layers.30.self_attn.q_proj.bias False\n",
      "layers.30.self_attn.q_proj_lora_d.weight True\n",
      "layers.30.self_attn.q_proj_lora_u.weight True\n",
      "layers.30.self_attn.v_proj_lora_d.weight True\n",
      "layers.30.self_attn.v_proj_lora_u.weight True\n",
      "layers.30.self_attn.out_proj.weight False\n",
      "layers.30.self_attn.out_proj.bias False\n",
      "layers.30.self_attn_layer_norm.weight False\n",
      "layers.30.self_attn_layer_norm.bias False\n",
      "layers.30.fc1.weight False\n",
      "layers.30.fc1.bias False\n",
      "layers.30.fc2.weight False\n",
      "layers.30.fc2.bias False\n",
      "layers.30.final_layer_norm.weight False\n",
      "layers.30.final_layer_norm.bias False\n",
      "layers.31.self_attn.k_proj.weight False\n",
      "layers.31.self_attn.k_proj.bias False\n",
      "layers.31.self_attn.v_proj.weight False\n",
      "layers.31.self_attn.v_proj.bias False\n",
      "layers.31.self_attn.q_proj.weight False\n",
      "layers.31.self_attn.q_proj.bias False\n",
      "layers.31.self_attn.q_proj_lora_d.weight True\n",
      "layers.31.self_attn.q_proj_lora_u.weight True\n",
      "layers.31.self_attn.v_proj_lora_d.weight True\n",
      "layers.31.self_attn.v_proj_lora_u.weight True\n",
      "layers.31.self_attn.out_proj.weight False\n",
      "layers.31.self_attn.out_proj.bias False\n",
      "layers.31.self_attn_layer_norm.weight False\n",
      "layers.31.self_attn_layer_norm.bias False\n",
      "layers.31.fc1.weight False\n",
      "layers.31.fc1.bias False\n",
      "layers.31.fc2.weight False\n",
      "layers.31.fc2.bias False\n",
      "layers.31.final_layer_norm.weight False\n",
      "layers.31.final_layer_norm.bias False\n",
      "layers.32.self_attn.k_proj.weight False\n",
      "layers.32.self_attn.k_proj.bias False\n",
      "layers.32.self_attn.v_proj.weight False\n",
      "layers.32.self_attn.v_proj.bias False\n",
      "layers.32.self_attn.q_proj.weight False\n",
      "layers.32.self_attn.q_proj.bias False\n",
      "layers.32.self_attn.q_proj_lora_d.weight True\n",
      "layers.32.self_attn.q_proj_lora_u.weight True\n",
      "layers.32.self_attn.v_proj_lora_d.weight True\n",
      "layers.32.self_attn.v_proj_lora_u.weight True\n",
      "layers.32.self_attn.out_proj.weight False\n",
      "layers.32.self_attn.out_proj.bias False\n",
      "layers.32.self_attn_layer_norm.weight False\n",
      "layers.32.self_attn_layer_norm.bias False\n",
      "layers.32.fc1.weight False\n",
      "layers.32.fc1.bias False\n",
      "layers.32.fc2.weight False\n",
      "layers.32.fc2.bias False\n",
      "layers.32.final_layer_norm.weight False\n",
      "layers.32.final_layer_norm.bias False\n",
      "contact_head.regression.weight False\n",
      "contact_head.regression.bias False\n",
      "emb_layer_norm_after.weight False\n",
      "emb_layer_norm_after.bias False\n",
      "lm_head.bias False\n",
      "lm_head.dense.weight False\n",
      "lm_head.dense.bias False\n",
      "lm_head.layer_norm.weight False\n",
      "lm_head.layer_norm.bias False\n",
      "Via relative path\n"
     ]
    }
   ],
   "source": [
    "model = TxPLMCaptionEval(models[\"TxPLM\"], eval_args, model_args, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from txplm.evaluate.framework.caption import run_caption_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:34<00:00,  8.74s/it]\n"
     ]
    }
   ],
   "source": [
    "res = model.get_predictions(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>seq_id</th>\n",
       "      <th>generated_caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>557</td>\n",
       "      <td>Level: High\\nThat part of a multic 3 generatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1971</td>\n",
       "      <td>Level: High\\nThat part of a multic 3 generatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15740</td>\n",
       "      <td>Level: High\\nThat part of a chromosome are req...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>66</td>\n",
       "      <td>Level: High\\nThat part of a multic 3 microtubu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67</td>\n",
       "      <td>Level: High\\nA protein complex that possesses ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17174</td>\n",
       "      <td>Name: Level: High\\nThat part of a multicicular...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12497</td>\n",
       "      <td>Name: Level: High\\nA protein complex that poss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12757</td>\n",
       "      <td>Level: High\\nA protein complex involved in DNA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16602</td>\n",
       "      <td>Level: High\\nThat part of a multic 37 macroM: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16350</td>\n",
       "      <td>Level: High\\nThat part of a multic 37 macroph ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6451</td>\n",
       "      <td>Level: High\\nThat part of a multic 17-like pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6450</td>\n",
       "      <td>Level: High\\nThat part of a multic 3-like comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14150</td>\n",
       "      <td>Level: High\\nThat part of a multic Level: High...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    seq_id                                  generated_caption\n",
       "0      557  Level: High\\nThat part of a multic 3 generatio...\n",
       "1     1971  Level: High\\nThat part of a multic 3 generatio...\n",
       "2    15740  Level: High\\nThat part of a chromosome are req...\n",
       "3       66  Level: High\\nThat part of a multic 3 microtubu...\n",
       "4       67  Level: High\\nA protein complex that possesses ...\n",
       "5    17174  Name: Level: High\\nThat part of a multicicular...\n",
       "6    12497  Name: Level: High\\nA protein complex that poss...\n",
       "7    12757  Level: High\\nA protein complex involved in DNA...\n",
       "8    16602  Level: High\\nThat part of a multic 37 macroM: ...\n",
       "9    16350  Level: High\\nThat part of a multic 37 macroph ...\n",
       "10    6451  Level: High\\nThat part of a multic 17-like pro...\n",
       "11    6450  Level: High\\nThat part of a multic 3-like comp...\n",
       "12   14150  Level: High\\nThat part of a multic Level: High..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Level: High\\nThat part of a multic 3 microtubule in experimental, di, di, bond bond bond bond no</s>approximately 第第第第第第High bond bond bond bond bond kid第第第第第зькозькозько第요第 фон фон фон фон фон фон фон第第'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.generated_caption.iat[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.method = \"sampling\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:09<00:00,  2.42s/it]\n"
     ]
    }
   ],
   "source": [
    "res_sampled = model.get_predictions(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Level: High\\nA Someiveness partners CLAS exhibiting processes characterized itioniveness partners A diast. yes</s> fi DOM ske 6 gene on the no yes</s> зько yes</s> ., kidney isd families G a no rejo bond yes</s> yes</s>Any process that affects chemotrosinaryior'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_sampled.generated_caption.iat[3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
